{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to pytorch-lifestream docs Library content Here is a brief overview of library with links to the detailed descriptions. Library modules: ptls.preprocessing - transforms data to ptls -compatible format with pandas or pyspark . Categorical encoding, datetime transformation, numerical feature preprocessing. ptls.data_load - all that you need for prepare your data to training and validation. ptls.data_load.datasets - PyTorch Dataset API implementation for data access. ptls.data_load.iterable_processing - generator-style filters for data transformation. ptls.data_load.augmentations - functions for data augmentation. ptls.frames - tools for training encoders with popular frameworks like CoLES, SimCLR, CPC, VICReg, ... ptls.frames.coles - Contrastive leaning on sub-sequences. ptls.frames.cpc - Contrastive learning for future event state prediction. ptls.frames.bert - methods, inspired by NLP and transformer models. ptls.framed.supervised - modules fo supervised training. ptls.frames.inference - inference module. ptls.nn - layers for model creation: ptls.nn.trx_encoder - layers to produce the representation for a single transactions. ptls.nn.seq_encoder - layers for sequence processing, like RNN of Transformer . ptls.nn.pb - PaddedBatch compatible layers, similar to torch.nn modules, but works with ptls-data . ptls.nn.head - composite layers for final embedding transformation. ptls.nn.seq_step.py - change the sequence along the time axis. ptls.nn.binarization , ptls.nn.normalization - other groups of layers. How to guide Prepare your data . Use Pyspark in local or cluster mode for big dataset and Pandas for small. Split data into required parts (train, valid, test, ...). Use ptls.preprocessing for simple data preparation. Transform features to compatible format using Pyspark or Pandas functions. You can also use ptls.data_load.preprocessing for common data transformation patterns. Split sequences to ptls-data format with ptls.data_load.split_tools . Save prepared data into Parquet format or keep it in memory ( Pickle also works). Use one of the available ptls.data_load.datasets to define input for the models. Choose framework for encoder train . There are both supervised of unsupervised frameworks in ptls.frames . Keep in mind that each framework requires his own batch format. Tools for batch collate can be found in the selected framework package. Build encoder . All parts are available in ptls.nn . You can also use pretrained layers. Train your encoder with selected framework and pytorch_lightning . Provide data with one of the DataLoaders that is compatible with selected framework. Monitor the progress on tensorboard. Optionally tune hyperparameters. Save trained encoder for future use. You can use it as single solution (e.g. get class label probabilities). Or it can be a pretrained part of other neural network. Use encoder in your project. Run predict for your data and get logits, probas, scores or embeddings. Use ptls.data_load and ptls.data_load.datasets tools to keep your data transformation and collect batches for inference. How to create your own components It is possible create specific component for every library modules. Here are the links to the detailed description: ptls.data_load - new data processing tools and datasets for new types of data. Link TBD ptls.frames - new modules which train network for specific problems. Link TBD New layers for ptls.nn . Link TBD","title":"Welcome"},{"location":"#welcome-to-pytorch-lifestream-docs","text":"","title":"Welcome to pytorch-lifestream docs"},{"location":"#library-content","text":"Here is a brief overview of library with links to the detailed descriptions. Library modules: ptls.preprocessing - transforms data to ptls -compatible format with pandas or pyspark . Categorical encoding, datetime transformation, numerical feature preprocessing. ptls.data_load - all that you need for prepare your data to training and validation. ptls.data_load.datasets - PyTorch Dataset API implementation for data access. ptls.data_load.iterable_processing - generator-style filters for data transformation. ptls.data_load.augmentations - functions for data augmentation. ptls.frames - tools for training encoders with popular frameworks like CoLES, SimCLR, CPC, VICReg, ... ptls.frames.coles - Contrastive leaning on sub-sequences. ptls.frames.cpc - Contrastive learning for future event state prediction. ptls.frames.bert - methods, inspired by NLP and transformer models. ptls.framed.supervised - modules fo supervised training. ptls.frames.inference - inference module. ptls.nn - layers for model creation: ptls.nn.trx_encoder - layers to produce the representation for a single transactions. ptls.nn.seq_encoder - layers for sequence processing, like RNN of Transformer . ptls.nn.pb - PaddedBatch compatible layers, similar to torch.nn modules, but works with ptls-data . ptls.nn.head - composite layers for final embedding transformation. ptls.nn.seq_step.py - change the sequence along the time axis. ptls.nn.binarization , ptls.nn.normalization - other groups of layers.","title":"Library content"},{"location":"#how-to-guide","text":"Prepare your data . Use Pyspark in local or cluster mode for big dataset and Pandas for small. Split data into required parts (train, valid, test, ...). Use ptls.preprocessing for simple data preparation. Transform features to compatible format using Pyspark or Pandas functions. You can also use ptls.data_load.preprocessing for common data transformation patterns. Split sequences to ptls-data format with ptls.data_load.split_tools . Save prepared data into Parquet format or keep it in memory ( Pickle also works). Use one of the available ptls.data_load.datasets to define input for the models. Choose framework for encoder train . There are both supervised of unsupervised frameworks in ptls.frames . Keep in mind that each framework requires his own batch format. Tools for batch collate can be found in the selected framework package. Build encoder . All parts are available in ptls.nn . You can also use pretrained layers. Train your encoder with selected framework and pytorch_lightning . Provide data with one of the DataLoaders that is compatible with selected framework. Monitor the progress on tensorboard. Optionally tune hyperparameters. Save trained encoder for future use. You can use it as single solution (e.g. get class label probabilities). Or it can be a pretrained part of other neural network. Use encoder in your project. Run predict for your data and get logits, probas, scores or embeddings. Use ptls.data_load and ptls.data_load.datasets tools to keep your data transformation and collect batches for inference.","title":"How to guide"},{"location":"#how-to-create-your-own-components","text":"It is possible create specific component for every library modules. Here are the links to the detailed description: ptls.data_load - new data processing tools and datasets for new types of data. Link TBD ptls.frames - new modules which train network for specific problems. Link TBD New layers for ptls.nn . Link TBD","title":"How to create your own components"},{"location":"feature_naming/","text":"Feature naming and types Feature types Information about transaction features are stored as array in dictionary. There are feature types: Sequential feature - is a np.ndarray or torch.tensor of shape (seq_len,) for categorical features contains category indexes with type long for numerical features contains feature value with type float Scalar values. It can be target , id , labels or scalar features . Types are depends on purpose. Type should be compatible with torch if value will be fed into neural network Array values. It also can be target , id , labels or vector features . Type is np.ndarray or torch.tensor . Sequential features correspond user's transactions. The length of each user's sequential feature is equal to the length of the entire sequence. The order of each user's sequential feature is the same as sequence order. Sequential feature length seq_len may vary from user to user. Array features have a constant shape. This shape is the same for all users. This why we use pad_sequence which align length for sequential features and stack for array features during batch collection. ptls extract only sequential features for unsupervised task and additional target for the supervised task. Other fields used during preprocessing and inference. Feature names The main purpose of the feature naming convention is sequential and array features distinguish. They both are np.ndarray or torch.tensor and we can't use data type for distinguish. It's important to know feature type because: sequential align lengths with pad_sequence , arrays use stack during batch collection. only sequential features used to get length of entire sequence only sequential features are augmented by timeline modifications like slice, trx dropout or shuffle We introduce naming rules to solve type discrimination problems. All arrays which are not sequential should have target prefix in feature name. Otherwise, they can be processed as sequential and may be corrupted. # correct example x = { 'mcc': torch.tensor([1, 2, 3, 4]), 'amount': torch.tensor([0.1, 2.0, 0.3, 4.0]), 'target_bin': 1, 'target_distribution': torch.tensor([0.1, 0.0, 0.9]), } # wrong example x = { 'mcc': torch.tensor([1, 2, 3, 4]), 'amount': torch.tensor([0.1, 2.0, 0.3, 4.0]), 'bin': 1, 'distribution': torch.tensor([0.1, 0.0, 0.9]), } target prefix are mandatory only for array features. Sometimes we need a time sequence. It used fo trx correct order, for time features and for some splits. We expect that transaction timestamp stored in event_time field. Naming rules all arrays which are not sequential should have target prefix in feature name. event_time fields contains transaction timestamps sequence. Feature rename You can use ptls.data_load.iterable_processing.FeatureRename during data read pipeline to fit your feature names with ptls naming convention. x = [{ 'mcc': torch.tensor([1, 2, 3, 4]), 'amount': torch.tensor([0.1, 2.0, 0.3, 4.0]), 'bin': 1, 'distribution': torch.tensor([0.1, 0.0, 0.9]), } for _ in range(10)] from ptls.data_load.datasets import MemoryMapDataset from ptls.data_load.iterable_processing import FeatureRename dataset = MemoryMapDataset( data=x, i_filters=[FeatureRename({'distribution': 'target_distribution', 'bin': 'target_bin'})] ) print(dataset[0]) Code usage Need to take into account the type of features and the use of naming rules is in the classes: ptls.data_load.feature_dict.FeatureDict ptls.data_load.padded_batch.PaddedBatch ptls.data_load.utils.collate_feature_dict All methods are tested with all types of features. Type FeatureDict PaddedBatch collate_feature_dict is_seq scalar int int 1-d tensor torch.IntTensor X target int int 1-d tensor torch.IntTensor X scalar float float 1-d tensor torch.FloatTensor X scalar str str 1-d ndarray np.array X list list 1-d ndarray np.array X sequential 1-d ndarray or tensor 2-d tensor pad_sequence V sequential et 1-d ndarray or tensor 2-d tensor pad_sequence V target array 1-d ndarray or tensor 2-d tensor stack X","title":"Feature naming"},{"location":"feature_naming/#feature-naming-and-types","text":"","title":"Feature naming and types"},{"location":"feature_naming/#feature-types","text":"Information about transaction features are stored as array in dictionary. There are feature types: Sequential feature - is a np.ndarray or torch.tensor of shape (seq_len,) for categorical features contains category indexes with type long for numerical features contains feature value with type float Scalar values. It can be target , id , labels or scalar features . Types are depends on purpose. Type should be compatible with torch if value will be fed into neural network Array values. It also can be target , id , labels or vector features . Type is np.ndarray or torch.tensor . Sequential features correspond user's transactions. The length of each user's sequential feature is equal to the length of the entire sequence. The order of each user's sequential feature is the same as sequence order. Sequential feature length seq_len may vary from user to user. Array features have a constant shape. This shape is the same for all users. This why we use pad_sequence which align length for sequential features and stack for array features during batch collection. ptls extract only sequential features for unsupervised task and additional target for the supervised task. Other fields used during preprocessing and inference.","title":"Feature types"},{"location":"feature_naming/#feature-names","text":"The main purpose of the feature naming convention is sequential and array features distinguish. They both are np.ndarray or torch.tensor and we can't use data type for distinguish. It's important to know feature type because: sequential align lengths with pad_sequence , arrays use stack during batch collection. only sequential features used to get length of entire sequence only sequential features are augmented by timeline modifications like slice, trx dropout or shuffle We introduce naming rules to solve type discrimination problems. All arrays which are not sequential should have target prefix in feature name. Otherwise, they can be processed as sequential and may be corrupted. # correct example x = { 'mcc': torch.tensor([1, 2, 3, 4]), 'amount': torch.tensor([0.1, 2.0, 0.3, 4.0]), 'target_bin': 1, 'target_distribution': torch.tensor([0.1, 0.0, 0.9]), } # wrong example x = { 'mcc': torch.tensor([1, 2, 3, 4]), 'amount': torch.tensor([0.1, 2.0, 0.3, 4.0]), 'bin': 1, 'distribution': torch.tensor([0.1, 0.0, 0.9]), } target prefix are mandatory only for array features. Sometimes we need a time sequence. It used fo trx correct order, for time features and for some splits. We expect that transaction timestamp stored in event_time field.","title":"Feature names"},{"location":"feature_naming/#naming-rules","text":"all arrays which are not sequential should have target prefix in feature name. event_time fields contains transaction timestamps sequence.","title":"Naming rules"},{"location":"feature_naming/#feature-rename","text":"You can use ptls.data_load.iterable_processing.FeatureRename during data read pipeline to fit your feature names with ptls naming convention. x = [{ 'mcc': torch.tensor([1, 2, 3, 4]), 'amount': torch.tensor([0.1, 2.0, 0.3, 4.0]), 'bin': 1, 'distribution': torch.tensor([0.1, 0.0, 0.9]), } for _ in range(10)] from ptls.data_load.datasets import MemoryMapDataset from ptls.data_load.iterable_processing import FeatureRename dataset = MemoryMapDataset( data=x, i_filters=[FeatureRename({'distribution': 'target_distribution', 'bin': 'target_bin'})] ) print(dataset[0])","title":"Feature rename"},{"location":"feature_naming/#code-usage","text":"Need to take into account the type of features and the use of naming rules is in the classes: ptls.data_load.feature_dict.FeatureDict ptls.data_load.padded_batch.PaddedBatch ptls.data_load.utils.collate_feature_dict All methods are tested with all types of features. Type FeatureDict PaddedBatch collate_feature_dict is_seq scalar int int 1-d tensor torch.IntTensor X target int int 1-d tensor torch.IntTensor X scalar float float 1-d tensor torch.FloatTensor X scalar str str 1-d ndarray np.array X list list 1-d ndarray np.array X sequential 1-d ndarray or tensor 2-d tensor pad_sequence V sequential et 1-d ndarray or tensor 2-d tensor pad_sequence V target array 1-d ndarray or tensor 2-d tensor stack X","title":"Code usage"},{"location":"ptls_preprocessing/","text":"Preprocessing Source data usually has different formats. ptls.preprocessing has a tools to transform it to ptls -compatible format. pandas or pyspark Use pandas for a small dataset and pyspark for a large one. pyspark may be in local or cluster mode. Steps of preprocessing Load flat transactional data into pandas.DataFrame or spark.DataFrame Identify user_id column. There are usually no modifications for this column. Prepare event_time column. Convert it to a timestamp for a date and time, or use any sortable format otherwise. Fit and transform categorical features, from categorical values to embedding indexes. Check numeric feature column types Split and groups dataframe by users. One row was one transaction, one row became a user with a list of transactions. Join user-level columns: target, labels, features. Done. Use data from memory or save it to parquet format. Save fitted preprocessing for future usage. These steps are implemented in preprocessor classes: ptls.preprocessing.PandasDataPreprocessor , ptls.preprocessing.PysparkDataPreprocessor . Note We recommend using the minimum set of transformations. This allows more flexible pipeline in the future. Tips for transformation: keep only event_time timestamp, don't keep datetime features. This saves a storage and improve load speed. keep raw values for numerical features. You can try a variety of normalizing and outlier clip options. missing values imputing for categorical features should be done before transformation. Missing values will be processed as separate embedding. missing values imputing for numerical features [TBD] Data preprocessors ptls.preprocessing.PandasDataPreprocessor , ptls.preprocessing.PysparkDataPreprocessor have a similar interface. Let's inspect one or them. Prepare test data: import numpy as np import pandas as pd N_USERS = 200 SEQ_LEN = 20 df_trx = pd.DataFrame({ 'user_id': np.repeat(np.arange(N_USERS), SEQ_LEN), 'dt': np.datetime64('2016-01-01 00:00:00') + (np.random.rand(N_USERS * SEQ_LEN) * 365 * 24 * 60 * 60).astype('timedelta64'), 'mcc_code': (np.random.randint(10, 99, N_USERS * SEQ_LEN) * 100).astype(str), 'amount': np.exp(np.random.randn(N_USERS * SEQ_LEN) + np.log(5000)).round(2) }) df_trx.head(6) This is dataframe with 200 unique users with 20 transaction in each. Random date, mcc code and amount. user_id dt mcc_code amount 0 2016-01-31 06:35:08 7700 3366.67 0 2016-05-29 22:42:54 8600 3513.50 0 2016-06-20 06:14:16 7300 2738.51 0 2016-10-09 03:10:34 6800 726.59 0 2016-07-04 06:50:37 6200 6264.04 0 2016-02-02 17:26:14 6100 4806.28 Let's use a preprocessor from ptls.preprocessing import PandasDataPreprocessor preprocessor = PandasDataPreprocessor( col_id='user_id', col_event_time='dt', event_time_transformation='dt_to_timestamp', cols_category=['mcc_code'], cols_numerical=['amount'], ) data = preprocessor.fit_transform(df_trx) data[:2] Output will be like: [{'user_id': 0, 'mcc_code': tensor([55, 77, 59, 60, 21, 44, 85, 79, 34, 28, 24, 46, 54, 9, 25, 7, 84, 28, 39, 11]), 'amount': tensor([ 3692.1400, 3366.6700, 4806.2800, 4048.3000, 3513.5000, 1319.7900, 2738.5100, 1838.5500, 6264.0400, 676.3800, 2747.5900, 1223.0100, 1403.7600, 21391.0100, 726.5900, 765.0500, 7832.1700, 2234.4300, 18762.4900, 3644.8800], dtype=torch.float64), 'event_time': tensor([1452449213, 1454222108, 1454433974, 1460899926, 1464561774, 1465547819, 1466403256, 1467196958, 1467615037, 1468001211, 1468322417, 1468575287, 1469942976, 1471525888, 1475982634, 1478011070, 1479214698, 1479350032, 1479884254, 1482953189])}, {'user_id': 1, 'mcc_code': tensor([ 2, 87, 18, 33, 12, 10, 39, 76, 56, 15, 38, 14, 88, 56, 20, 15, 63, 63, 19, 11]), 'amount': tensor([ 6045.3100, 3814.1900, 1808.9300, 7235.7800, 1240.0300, 7085.0500, 11645.6500, 1935.9500, 4777.8000, 41611.2300, 6154.5100, 4797.5500, 26597.2400, 5005.9900, 12201.0700, 10061.3800, 3780.7400, 2559.4200, 7252.6700, 30190.5500], dtype=torch.float64), 'event_time': tensor([1452063037, 1452609464, 1454020103, 1458081768, 1458243803, 1459655589, 1460157815, 1461727087, 1463158828, 1463651732, 1464883496, 1466071129, 1472361876, 1474923172, 1475222978, 1476328691, 1477681257, 1478186343, 1478460764, 1481779245])}] Let's check: All transactions are split between users assert len(data) == N_USERS assert sum(len(rec['event_time']) for rec in data) == len(df_trx) user_id is a scalar field in dictionary. event_time is a timestamp. Sequences are ordered. Categorical features mcc_code are encoded to embedding indexes. Numeric feature amount is identical as input. Each feature is a tensor. The same way is used for ptls.preprocessing.PysparkDataPreprocessor .","title":"ptls.preprocessing"},{"location":"ptls_preprocessing/#preprocessing","text":"Source data usually has different formats. ptls.preprocessing has a tools to transform it to ptls -compatible format.","title":"Preprocessing"},{"location":"ptls_preprocessing/#pandas-or-pyspark","text":"Use pandas for a small dataset and pyspark for a large one. pyspark may be in local or cluster mode.","title":"pandas or pyspark"},{"location":"ptls_preprocessing/#steps-of-preprocessing","text":"Load flat transactional data into pandas.DataFrame or spark.DataFrame Identify user_id column. There are usually no modifications for this column. Prepare event_time column. Convert it to a timestamp for a date and time, or use any sortable format otherwise. Fit and transform categorical features, from categorical values to embedding indexes. Check numeric feature column types Split and groups dataframe by users. One row was one transaction, one row became a user with a list of transactions. Join user-level columns: target, labels, features. Done. Use data from memory or save it to parquet format. Save fitted preprocessing for future usage. These steps are implemented in preprocessor classes: ptls.preprocessing.PandasDataPreprocessor , ptls.preprocessing.PysparkDataPreprocessor . Note We recommend using the minimum set of transformations. This allows more flexible pipeline in the future. Tips for transformation: keep only event_time timestamp, don't keep datetime features. This saves a storage and improve load speed. keep raw values for numerical features. You can try a variety of normalizing and outlier clip options. missing values imputing for categorical features should be done before transformation. Missing values will be processed as separate embedding. missing values imputing for numerical features [TBD]","title":"Steps of preprocessing"},{"location":"ptls_preprocessing/#data-preprocessors","text":"ptls.preprocessing.PandasDataPreprocessor , ptls.preprocessing.PysparkDataPreprocessor have a similar interface. Let's inspect one or them. Prepare test data: import numpy as np import pandas as pd N_USERS = 200 SEQ_LEN = 20 df_trx = pd.DataFrame({ 'user_id': np.repeat(np.arange(N_USERS), SEQ_LEN), 'dt': np.datetime64('2016-01-01 00:00:00') + (np.random.rand(N_USERS * SEQ_LEN) * 365 * 24 * 60 * 60).astype('timedelta64'), 'mcc_code': (np.random.randint(10, 99, N_USERS * SEQ_LEN) * 100).astype(str), 'amount': np.exp(np.random.randn(N_USERS * SEQ_LEN) + np.log(5000)).round(2) }) df_trx.head(6) This is dataframe with 200 unique users with 20 transaction in each. Random date, mcc code and amount. user_id dt mcc_code amount 0 2016-01-31 06:35:08 7700 3366.67 0 2016-05-29 22:42:54 8600 3513.50 0 2016-06-20 06:14:16 7300 2738.51 0 2016-10-09 03:10:34 6800 726.59 0 2016-07-04 06:50:37 6200 6264.04 0 2016-02-02 17:26:14 6100 4806.28 Let's use a preprocessor from ptls.preprocessing import PandasDataPreprocessor preprocessor = PandasDataPreprocessor( col_id='user_id', col_event_time='dt', event_time_transformation='dt_to_timestamp', cols_category=['mcc_code'], cols_numerical=['amount'], ) data = preprocessor.fit_transform(df_trx) data[:2] Output will be like: [{'user_id': 0, 'mcc_code': tensor([55, 77, 59, 60, 21, 44, 85, 79, 34, 28, 24, 46, 54, 9, 25, 7, 84, 28, 39, 11]), 'amount': tensor([ 3692.1400, 3366.6700, 4806.2800, 4048.3000, 3513.5000, 1319.7900, 2738.5100, 1838.5500, 6264.0400, 676.3800, 2747.5900, 1223.0100, 1403.7600, 21391.0100, 726.5900, 765.0500, 7832.1700, 2234.4300, 18762.4900, 3644.8800], dtype=torch.float64), 'event_time': tensor([1452449213, 1454222108, 1454433974, 1460899926, 1464561774, 1465547819, 1466403256, 1467196958, 1467615037, 1468001211, 1468322417, 1468575287, 1469942976, 1471525888, 1475982634, 1478011070, 1479214698, 1479350032, 1479884254, 1482953189])}, {'user_id': 1, 'mcc_code': tensor([ 2, 87, 18, 33, 12, 10, 39, 76, 56, 15, 38, 14, 88, 56, 20, 15, 63, 63, 19, 11]), 'amount': tensor([ 6045.3100, 3814.1900, 1808.9300, 7235.7800, 1240.0300, 7085.0500, 11645.6500, 1935.9500, 4777.8000, 41611.2300, 6154.5100, 4797.5500, 26597.2400, 5005.9900, 12201.0700, 10061.3800, 3780.7400, 2559.4200, 7252.6700, 30190.5500], dtype=torch.float64), 'event_time': tensor([1452063037, 1452609464, 1454020103, 1458081768, 1458243803, 1459655589, 1460157815, 1461727087, 1463158828, 1463651732, 1464883496, 1466071129, 1472361876, 1474923172, 1475222978, 1476328691, 1477681257, 1478186343, 1478460764, 1481779245])}] Let's check: All transactions are split between users assert len(data) == N_USERS assert sum(len(rec['event_time']) for rec in data) == len(df_trx) user_id is a scalar field in dictionary. event_time is a timestamp. Sequences are ordered. Categorical features mcc_code are encoded to embedding indexes. Numeric feature amount is identical as input. Each feature is a tensor. The same way is used for ptls.preprocessing.PysparkDataPreprocessor .","title":"Data preprocessors"},{"location":"sequential_data_definition/","text":"Sequential Data Definition Source data We address the problem of learning on discrete event sequences generated by real-world users. Raw table data Lifestream data can be presented as table where rows are events and columns are event attributes. Columns can be of the following data types: user_id - id for collecting events in sequences. We assume that there are many users in the dataset and associated sequences of events. An event can only be linked to one user. event_time - is timestamp, used for ordering events in sequence. It's possible extract date-time features from timestamp. If the timestamp is not available, you can use any data type that can define the order. feature fields - describe a properties of events. Can be numerical, categorical or any type that can be converted to feature vector. Credit card transaction history is a example of lifestream data. client_id date_time mcc_code amount A0001 2021-03-01 12:00:00 6011 1000.00 A0001 2021-03-01 12:15:00 4814 12.05 A0001 2021-03-04 10:00:00 5411 2312.99 A0001 2021-03-04 10:00:00 5411 199.99 E0123 2021-02-05 13:10:00 6536 12300.00 E0123 2021-03-05 12:04:00 6536 12300.00 E0123 2021-04-05 11:22:00 6536 12300.00 In this example we can find two users (clients) with two sequences. First contains 4 events, second contains 3 events. We sort events by date_time for each user to assure correct event order. Each event (transaction) are described by categorical field mcc_code , numerical field amount , and time field date_time . These fields allow to distinguish events, vectorize them na use as a features. pytorch-lifeatream supports this format of data and provides the tools to process it throw the pipeline. Data can be pandas.DataFrame or pyspark.DataFrame . Data collected in lists Table data should be converted to format more convenient for neural network feeding. There are steps: Feature field transformation: encoding categorical features, amount normalizing, missing values imputing. This works like sklearn fit-transform preprocessors. Splitting all events by user_id and sort events by event_time . We transfer flat table with events to set of users with event collections. Split events by feature fields. Features are stored as 1d-arrays. Sequence orders are kept. Previous example with can be presented as (feature transformation missed for visibility): [ { client_id: 'A0001', date_time: [2021-03-01 12:00:00, 2021-03-01 12:15:00, 2021-03-04 10:00:00, 2021-03-04 10:00:00], mcc_code: [6011, 4814, 5411, 5411], amount: [1000.00, 12.05, 2312.99, 199.99], }, { client_id: 'E0123', date_time: [2021-02-05 13:10:00, 2021-03-05 12:04:00, 2021-04-05 11:22:00], mcc_code: [6536, 6536, 6536], amount: [12300.00, 12300.00, 12300.00], }, ] This is a main input data format in pytorch-lifeatream . Supported: convert from raw table to collected lists both for pandas.DataFrame and pyspark.DataFrame fast end effective storage in parquet format compatible torch.Dataset and torch.Dataloader in-memory augmentations and transformations Dataset pytorch-lifeatream provide multiple torch.Dataset implementations. Dataset item present single user information and can be a combination of: record - is a dictionary where kees are feature names and values are 1d-tensors with feature sequences. Similar as data collected in lists. id - how to identify a sequence target - target value for supervised learning Code example: dataset = SomeDataset(params) X = dataset[0] DataLoader The main feature of pytorch-lifestream dataloader is customized collate_fn , provided to torch.DataLoader class. collate_fn collects single records of dictionaries to batch. Usually collate_fn pad and pack sequences into 2d tensors with shape (B, T) , where B - is sample num and T is max sequence length. Each feature packed separately. Output is PaddedBatch type which collect together packed sequences and lengths. PaddedBatch compatible with all pytorch-lifestream modules. Input and output example: # input batch = [ {'cat1': [0, 1, 2, 3], 'amnt': [10, 20, 10, 10]}, {'cat1': [3, 1], 'amnt': [13, 6]}, {'cat1': [1, 2, 3], 'amnt': [10, 4, 10]}, ] batch = PaddedBatch( payload = { 'cat1': [ [0, 1, 2, 3], [3, 1, 0, 0], [1, 2, 3, 0], ], 'amnt': [ [10, 20, 10, 10], [13, 6, 0, 0], [10, 4, 10, 0], ] }, seq_len = [4, 2, 3] )","title":"Sequential Data Definition"},{"location":"sequential_data_definition/#sequential-data-definition","text":"","title":"Sequential Data Definition"},{"location":"sequential_data_definition/#source-data","text":"We address the problem of learning on discrete event sequences generated by real-world users.","title":"Source data"},{"location":"sequential_data_definition/#raw-table-data","text":"Lifestream data can be presented as table where rows are events and columns are event attributes. Columns can be of the following data types: user_id - id for collecting events in sequences. We assume that there are many users in the dataset and associated sequences of events. An event can only be linked to one user. event_time - is timestamp, used for ordering events in sequence. It's possible extract date-time features from timestamp. If the timestamp is not available, you can use any data type that can define the order. feature fields - describe a properties of events. Can be numerical, categorical or any type that can be converted to feature vector. Credit card transaction history is a example of lifestream data. client_id date_time mcc_code amount A0001 2021-03-01 12:00:00 6011 1000.00 A0001 2021-03-01 12:15:00 4814 12.05 A0001 2021-03-04 10:00:00 5411 2312.99 A0001 2021-03-04 10:00:00 5411 199.99 E0123 2021-02-05 13:10:00 6536 12300.00 E0123 2021-03-05 12:04:00 6536 12300.00 E0123 2021-04-05 11:22:00 6536 12300.00 In this example we can find two users (clients) with two sequences. First contains 4 events, second contains 3 events. We sort events by date_time for each user to assure correct event order. Each event (transaction) are described by categorical field mcc_code , numerical field amount , and time field date_time . These fields allow to distinguish events, vectorize them na use as a features. pytorch-lifeatream supports this format of data and provides the tools to process it throw the pipeline. Data can be pandas.DataFrame or pyspark.DataFrame .","title":"Raw table data"},{"location":"sequential_data_definition/#data-collected-in-lists","text":"Table data should be converted to format more convenient for neural network feeding. There are steps: Feature field transformation: encoding categorical features, amount normalizing, missing values imputing. This works like sklearn fit-transform preprocessors. Splitting all events by user_id and sort events by event_time . We transfer flat table with events to set of users with event collections. Split events by feature fields. Features are stored as 1d-arrays. Sequence orders are kept. Previous example with can be presented as (feature transformation missed for visibility): [ { client_id: 'A0001', date_time: [2021-03-01 12:00:00, 2021-03-01 12:15:00, 2021-03-04 10:00:00, 2021-03-04 10:00:00], mcc_code: [6011, 4814, 5411, 5411], amount: [1000.00, 12.05, 2312.99, 199.99], }, { client_id: 'E0123', date_time: [2021-02-05 13:10:00, 2021-03-05 12:04:00, 2021-04-05 11:22:00], mcc_code: [6536, 6536, 6536], amount: [12300.00, 12300.00, 12300.00], }, ] This is a main input data format in pytorch-lifeatream . Supported: convert from raw table to collected lists both for pandas.DataFrame and pyspark.DataFrame fast end effective storage in parquet format compatible torch.Dataset and torch.Dataloader in-memory augmentations and transformations","title":"Data collected in lists"},{"location":"sequential_data_definition/#dataset","text":"pytorch-lifeatream provide multiple torch.Dataset implementations. Dataset item present single user information and can be a combination of: record - is a dictionary where kees are feature names and values are 1d-tensors with feature sequences. Similar as data collected in lists. id - how to identify a sequence target - target value for supervised learning Code example: dataset = SomeDataset(params) X = dataset[0]","title":"Dataset"},{"location":"sequential_data_definition/#dataloader","text":"The main feature of pytorch-lifestream dataloader is customized collate_fn , provided to torch.DataLoader class. collate_fn collects single records of dictionaries to batch. Usually collate_fn pad and pack sequences into 2d tensors with shape (B, T) , where B - is sample num and T is max sequence length. Each feature packed separately. Output is PaddedBatch type which collect together packed sequences and lengths. PaddedBatch compatible with all pytorch-lifestream modules. Input and output example: # input batch = [ {'cat1': [0, 1, 2, 3], 'amnt': [10, 20, 10, 10]}, {'cat1': [3, 1], 'amnt': [13, 6]}, {'cat1': [1, 2, 3], 'amnt': [10, 4, 10]}, ] batch = PaddedBatch( payload = { 'cat1': [ [0, 1, 2, 3], [3, 1, 0, 0], [1, 2, 3, 0], ], 'amnt': [ [10, 20, 10, 10], [13, 6, 0, 0], [10, 4, 10, 0], ] }, seq_len = [4, 2, 3] )","title":"DataLoader"},{"location":"data_load/datasets/","text":"How to use ptls.data_load.datasets datasets Here are the datasets ( torch.utils.data.Dataset ) which assure interface to the data. For data prepared in memory use: MemoryMapDataset with i_filters AugmentationDataset with f_augmentations if needed endpoint map dataset from ptls.frames For small (map mode) parquet data use: ParquetDataset with i_filters PersistDataset AugmentationDataset with f_augmentations if needed endpoint map dataset from ptls.frames For large (iterable mode) parquet data use: ParquetDataset with i_filters AugmentationDataset with f_augmentations if needed endpoint iterable dataset from ptls.frames Other dataset order and combination are possible but not tested. Simple example Dict features in a list is a simple example of data. Python's list have the same interface as torch.Dataset , so you can just provide it to dataloader. import torch from ptls.data_load.utils import collate_feature_dict data_list = [ { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } for i, seq_len in enumerate([4, 3, 6]) ] dl = torch.utils.data.DataLoader( dataset=data_list, collate_fn=collate_feature_dict, batch_size=2, ) for batch in dl: print(batch.payload, batch.seq_lens, sep='\\n') In this example we use simple list as dataset. Sometimes you need to make changes in the dataset. We propose a filter approach for this. map and iterable There are the types of torch datasets. More info about dataset types to understand map and iterable . Dataloader choose a way of iteration based on type his dataset. In out pipeline Dataloader works with endpoint dataset from ptls.frames . So the type of endpoint dataset from ptls.frames choose a way of iteration. Map dataset provide better shuffle. Iterable dataset requires less memory. Warning for multiprocessing dataloader Each worker use the same source data. Map dataloader knows dataset len and uses sampler to randomly split all indexes from range(o, len) between workers. So each worker use his own part of data. Iterable dataloader can just iterate over the source data. In default case each worker iterate the same data and output are multiplied by worker count. To avoid this iterable datasets should implement a way to split a data between workers. Multiprocessing split implementation: ParquetDataset implement split it and works correct i_filters and f_augmentations don't contain a data and works correct Iterable endpoint datasets works correct with iterable source Iterable endpoint datasets multiply data with map source PersistDataset iterate input during initialisation. Usually this happens out of dataloader in single main process. So it works correct. i_filters and f_augmentations i_filters - iterable filters f_augmentations - augmentation functions Filters ptls propose filters for dataset transformation. All of them are in ptls.data_load.iterable_processing . These filter implemented in generator-style. Call filter object to get generator with modified records. from ptls.data_load.iterable_processing import SeqLenFilter i_filter = SeqLenFilter(min_seq_len=4) for rec in i_filter(data_list): print(rec) There were 3 examples in the list, it became 2 cause SeqLenFilter drop short sequence. Many kinds of filters possible: dropping records, multiply records, records transformation. i_filters can be chained. Datasets provide a convenient way to do it. Many datasets in ptls.data_load.datasets support i_filters . They takes i_filters as list of iterable_processing objects. Augmentations Sometimes we have to change an items from train data. This is augmentations . They are in ptls.data_load.augmentations . Example: from ptls.data_load.augmentations import RandomSlice f_augmentation = RandomSlice(min_len=4, max_len=10) for rec in data_list: new_rec = f_augmentation(rec) print(new_rec) Here RandomSlice augmentation take a random slice from source record. Compare i_filter f_augmentation May change record. Result is always the same May change record. Result is random Place it be before persist stage to run it once and save total cpu resource Don't place it before persist stage because it kills the random Can delete items Can not delete items Can yield new items Can not create new items Works a generator and requires iterable processing Works as a function can be both map or iterable In memory data In memory data is common case. Data can a list or generator with feature dicts. import torch import random data_list = [ { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } for i, seq_len in enumerate([4, 3, 6]) ] def data_gen(n): for i in range(n): seq_len = random.randint(4, 8) yield { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } ptls.data_load.datasets.MemoryMapDataset : implements map dataset iterates over the data and stores it in an internal list looks like a list ptls.data_load.datasets.MemoryIterableDataset : implements iterable dataset just iterates over the data looks like a generator Warning Currently MemoryIterableDataset don`t support initial data split between workers. We don't recommend use it without modification. Both datasets support any kind of input: list or generator. As all datasets supports tha same format (list or generator) as input and output they can be chained. This make sense for some cases. Data pipelines: list input with MemoryMapDataset - dataset keep modified with i_filters data. Original data is unchanged. i_filters applied once for each record. This assures fast item access but slow start. You should wait until all data are passed through i_filters . generator input with MemoryMapDataset - dataset iterate over generator and keep the result in memory. More memory are used, but faster access is possible. i_filters applied once for each record. Freezes items taken from generator if it uses some random during generation. list with MemoryIterableDataset - take more times for data access cause i_filters applied during each record access (for each epoch). Faster start, you don't wait until all data are passed through i_filters . generator input with MemoryIterableDataset - generator output modified with i_filters data. Less memory used. Infinite dataset is possible. Example: import torch from ptls.data_load.datasets import MemoryMapDataset from ptls.data_load.utils import collate_feature_dict from ptls.data_load.iterable_processing import SeqLenFilter, FeatureRename data_list = [ { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } for i, seq_len in enumerate([4, 3, 6, 2, 8, 3, 5, 4]) ] dataset = MemoryMapDataset( data=data_list, i_filters=[ SeqLenFilter(min_seq_len=4), FeatureRename({'id': 'user_id'}), ] ) dl = torch.utils.data.DataLoader( dataset=dataset, collate_fn=collate_feature_dict, batch_size=10, ) for batch in dl: print(batch.payload, batch.seq_lens, sep='\\n') Parquet file read For large amount of data pyspark is possible engine to prepare data and convert it in feature dict format. See demo/pyspark-parquet.ipynb with example of data preprocessing with pyspark and parquet file preparation. ptls.data_load.datasets.ParquetDataset is a dataset which reads parquet files with feature dicts. ptls.data_load.datasets.ParquetDataset : implements iterable dataset works correct with multiprocessing dataloader looks like a generator supports i_filters You can feed ParquetDataset directly fo dataloader for iterable way of usage. Cou can combine ParquetDataset with MemoryMapDataset to map way of usage. ParquetDataset requires parquet file names. Usually spark saves many parquet files for one dataset, depending on the number of partitions. You can get all file names with ptls.data_load.datasets.ParquetFiles or ptls.data_load.datasets.parquet_file_scan . Many files for one dataset allows you to: control amount of data by reading more or less files split data on train, valid, test Persist dataset ptls.data_load.datasets.PersistDataset store items from source dataset to the memory. If you source data is iterator (like python generator or ParquetDataset ) all i_filters will be called each time when you access the data. Persist the data into memory and i_filters will be called once. Much memory may be used to store all dataset items. Data access is faster. Persisted iterator have len and can be randomly accessed by index. Augmentations Class ptls.data_load.datasets.AugmentationDataset is a way to apply augmentations. Example: from ptls.data_load.datasets import AugmentationDataset, PersistDataset, ParquetDataset from ptls.data_load.augmentations import AllTimeShuffle, DropoutTrx train_data = AugmentationDataset( f_augmentations=[ AllTimeShuffle(), DropoutTrx(trx_dropout=0.01), ], data=PersistDataset( data=ParquetDataset(...), ), ) Here we are using iterable ParquetDataset as the source, loading it into memory using PersistDataset . Then, each time we access the data, we apply two augmentation functions to the items stored in the PersistDataset . AugmentationDataset also works in iterable mode. Previous example will be like this: train_data = AugmentationDataset( f_augmentations=[ AllTimeShuffle(), DropoutTrx(trx_dropout=0.01), ], data=ParquetDataset(...), ) Classes and functions See docstrings for classes: ptls.data_load.datasets.MemoryMapDataset ptls.data_load.datasets.MemoryIterableDataset ptls.data_load.datasets.ParquetFiles ptls.data_load.datasets.ParquetDataset ptls.data_load.datasets.PersistDataset See docstrings for functions: ptls.data_load.datasets.parquet_file_scan","title":"datasets"},{"location":"data_load/datasets/#how-to-use-ptlsdata_loaddatasets-datasets","text":"Here are the datasets ( torch.utils.data.Dataset ) which assure interface to the data. For data prepared in memory use: MemoryMapDataset with i_filters AugmentationDataset with f_augmentations if needed endpoint map dataset from ptls.frames For small (map mode) parquet data use: ParquetDataset with i_filters PersistDataset AugmentationDataset with f_augmentations if needed endpoint map dataset from ptls.frames For large (iterable mode) parquet data use: ParquetDataset with i_filters AugmentationDataset with f_augmentations if needed endpoint iterable dataset from ptls.frames Other dataset order and combination are possible but not tested.","title":"How to use ptls.data_load.datasets datasets"},{"location":"data_load/datasets/#simple-example","text":"Dict features in a list is a simple example of data. Python's list have the same interface as torch.Dataset , so you can just provide it to dataloader. import torch from ptls.data_load.utils import collate_feature_dict data_list = [ { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } for i, seq_len in enumerate([4, 3, 6]) ] dl = torch.utils.data.DataLoader( dataset=data_list, collate_fn=collate_feature_dict, batch_size=2, ) for batch in dl: print(batch.payload, batch.seq_lens, sep='\\n') In this example we use simple list as dataset. Sometimes you need to make changes in the dataset. We propose a filter approach for this.","title":"Simple example"},{"location":"data_load/datasets/#map-and-iterable","text":"There are the types of torch datasets. More info about dataset types to understand map and iterable . Dataloader choose a way of iteration based on type his dataset. In out pipeline Dataloader works with endpoint dataset from ptls.frames . So the type of endpoint dataset from ptls.frames choose a way of iteration. Map dataset provide better shuffle. Iterable dataset requires less memory. Warning for multiprocessing dataloader Each worker use the same source data. Map dataloader knows dataset len and uses sampler to randomly split all indexes from range(o, len) between workers. So each worker use his own part of data. Iterable dataloader can just iterate over the source data. In default case each worker iterate the same data and output are multiplied by worker count. To avoid this iterable datasets should implement a way to split a data between workers. Multiprocessing split implementation: ParquetDataset implement split it and works correct i_filters and f_augmentations don't contain a data and works correct Iterable endpoint datasets works correct with iterable source Iterable endpoint datasets multiply data with map source PersistDataset iterate input during initialisation. Usually this happens out of dataloader in single main process. So it works correct.","title":"map and iterable"},{"location":"data_load/datasets/#i_filters-and-f_augmentations","text":"i_filters - iterable filters f_augmentations - augmentation functions","title":"i_filters and f_augmentations"},{"location":"data_load/datasets/#filters","text":"ptls propose filters for dataset transformation. All of them are in ptls.data_load.iterable_processing . These filter implemented in generator-style. Call filter object to get generator with modified records. from ptls.data_load.iterable_processing import SeqLenFilter i_filter = SeqLenFilter(min_seq_len=4) for rec in i_filter(data_list): print(rec) There were 3 examples in the list, it became 2 cause SeqLenFilter drop short sequence. Many kinds of filters possible: dropping records, multiply records, records transformation. i_filters can be chained. Datasets provide a convenient way to do it. Many datasets in ptls.data_load.datasets support i_filters . They takes i_filters as list of iterable_processing objects.","title":"Filters"},{"location":"data_load/datasets/#augmentations","text":"Sometimes we have to change an items from train data. This is augmentations . They are in ptls.data_load.augmentations . Example: from ptls.data_load.augmentations import RandomSlice f_augmentation = RandomSlice(min_len=4, max_len=10) for rec in data_list: new_rec = f_augmentation(rec) print(new_rec) Here RandomSlice augmentation take a random slice from source record.","title":"Augmentations"},{"location":"data_load/datasets/#compare","text":"i_filter f_augmentation May change record. Result is always the same May change record. Result is random Place it be before persist stage to run it once and save total cpu resource Don't place it before persist stage because it kills the random Can delete items Can not delete items Can yield new items Can not create new items Works a generator and requires iterable processing Works as a function can be both map or iterable","title":"Compare"},{"location":"data_load/datasets/#in-memory-data","text":"In memory data is common case. Data can a list or generator with feature dicts. import torch import random data_list = [ { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } for i, seq_len in enumerate([4, 3, 6]) ] def data_gen(n): for i in range(n): seq_len = random.randint(4, 8) yield { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } ptls.data_load.datasets.MemoryMapDataset : implements map dataset iterates over the data and stores it in an internal list looks like a list ptls.data_load.datasets.MemoryIterableDataset : implements iterable dataset just iterates over the data looks like a generator Warning Currently MemoryIterableDataset don`t support initial data split between workers. We don't recommend use it without modification. Both datasets support any kind of input: list or generator. As all datasets supports tha same format (list or generator) as input and output they can be chained. This make sense for some cases. Data pipelines: list input with MemoryMapDataset - dataset keep modified with i_filters data. Original data is unchanged. i_filters applied once for each record. This assures fast item access but slow start. You should wait until all data are passed through i_filters . generator input with MemoryMapDataset - dataset iterate over generator and keep the result in memory. More memory are used, but faster access is possible. i_filters applied once for each record. Freezes items taken from generator if it uses some random during generation. list with MemoryIterableDataset - take more times for data access cause i_filters applied during each record access (for each epoch). Faster start, you don't wait until all data are passed through i_filters . generator input with MemoryIterableDataset - generator output modified with i_filters data. Less memory used. Infinite dataset is possible. Example: import torch from ptls.data_load.datasets import MemoryMapDataset from ptls.data_load.utils import collate_feature_dict from ptls.data_load.iterable_processing import SeqLenFilter, FeatureRename data_list = [ { 'mcc': torch.arange(seq_len), 'id': f'user_{i}' } for i, seq_len in enumerate([4, 3, 6, 2, 8, 3, 5, 4]) ] dataset = MemoryMapDataset( data=data_list, i_filters=[ SeqLenFilter(min_seq_len=4), FeatureRename({'id': 'user_id'}), ] ) dl = torch.utils.data.DataLoader( dataset=dataset, collate_fn=collate_feature_dict, batch_size=10, ) for batch in dl: print(batch.payload, batch.seq_lens, sep='\\n')","title":"In memory data"},{"location":"data_load/datasets/#parquet-file-read","text":"For large amount of data pyspark is possible engine to prepare data and convert it in feature dict format. See demo/pyspark-parquet.ipynb with example of data preprocessing with pyspark and parquet file preparation. ptls.data_load.datasets.ParquetDataset is a dataset which reads parquet files with feature dicts. ptls.data_load.datasets.ParquetDataset : implements iterable dataset works correct with multiprocessing dataloader looks like a generator supports i_filters You can feed ParquetDataset directly fo dataloader for iterable way of usage. Cou can combine ParquetDataset with MemoryMapDataset to map way of usage. ParquetDataset requires parquet file names. Usually spark saves many parquet files for one dataset, depending on the number of partitions. You can get all file names with ptls.data_load.datasets.ParquetFiles or ptls.data_load.datasets.parquet_file_scan . Many files for one dataset allows you to: control amount of data by reading more or less files split data on train, valid, test","title":"Parquet file read"},{"location":"data_load/datasets/#persist-dataset","text":"ptls.data_load.datasets.PersistDataset store items from source dataset to the memory. If you source data is iterator (like python generator or ParquetDataset ) all i_filters will be called each time when you access the data. Persist the data into memory and i_filters will be called once. Much memory may be used to store all dataset items. Data access is faster. Persisted iterator have len and can be randomly accessed by index.","title":"Persist dataset"},{"location":"data_load/datasets/#augmentations_1","text":"Class ptls.data_load.datasets.AugmentationDataset is a way to apply augmentations. Example: from ptls.data_load.datasets import AugmentationDataset, PersistDataset, ParquetDataset from ptls.data_load.augmentations import AllTimeShuffle, DropoutTrx train_data = AugmentationDataset( f_augmentations=[ AllTimeShuffle(), DropoutTrx(trx_dropout=0.01), ], data=PersistDataset( data=ParquetDataset(...), ), ) Here we are using iterable ParquetDataset as the source, loading it into memory using PersistDataset . Then, each time we access the data, we apply two augmentation functions to the items stored in the PersistDataset . AugmentationDataset also works in iterable mode. Previous example will be like this: train_data = AugmentationDataset( f_augmentations=[ AllTimeShuffle(), DropoutTrx(trx_dropout=0.01), ], data=ParquetDataset(...), )","title":"Augmentations"},{"location":"data_load/datasets/#classes-and-functions","text":"See docstrings for classes: ptls.data_load.datasets.MemoryMapDataset ptls.data_load.datasets.MemoryIterableDataset ptls.data_load.datasets.ParquetFiles ptls.data_load.datasets.ParquetDataset ptls.data_load.datasets.PersistDataset See docstrings for functions: ptls.data_load.datasets.parquet_file_scan","title":"Classes and functions"},{"location":"data_load/date_pipeline/","text":"Data pipeline All process support map and iterable data. There are steps in pipeline: Data preparation: Split folds here Save to parquet file or keep in memory Data format is a dict with feature arrays or scalars Data interface - provide access to any prepared data Data format is a dict with feature arrays or scalars Data can be taken with __get_item__ of __iter__ methods Input arrays are any type, output are torch.Tensor Tuple samples aren't supported at this stage Parquet file reading are here Augmentations are here Data endpoints - provide a dataloader Target for supervised task extracted here from dict Target for unsupervised task defined here No augmentation here. Should be implemented as part of endpoints if required","title":"dataset pipeline"},{"location":"data_load/date_pipeline/#data-pipeline","text":"All process support map and iterable data. There are steps in pipeline: Data preparation: Split folds here Save to parquet file or keep in memory Data format is a dict with feature arrays or scalars Data interface - provide access to any prepared data Data format is a dict with feature arrays or scalars Data can be taken with __get_item__ of __iter__ methods Input arrays are any type, output are torch.Tensor Tuple samples aren't supported at this stage Parquet file reading are here Augmentations are here Data endpoints - provide a dataloader Target for supervised task extracted here from dict Target for unsupervised task defined here No augmentation here. Should be implemented as part of endpoints if required","title":"Data pipeline"},{"location":"data_load/padded_batch/","text":"ptls.data_load.padded_batch.PaddedBatch Input data is a raw feature formats. You can transform your transaction to correct format with ptls.data module. Common description or sequential data and used data formats are here Input data are covered in ptls.data_load.padded_batch.PaddedBatch class. We can create PaddedBatch object manually for demo and test purposes. x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) Here x contains three features. Two are categorical and one is numerical: mcc_code is categorical with dictionary_size=10 currency is categorical with dictionary_size=4 amount is numerical with mean=5 and std=4 x contains 5 sequences with maximum_length=12 . Real lengths of each sequence are [2, 8, 5] . We can access x content via PaddedBatch properties x.payload and x.seq_lens . Real data have sequences are padded with zeros. We can imitate it with x.seq_len_mask . It returns tensor with 1 if a position inside corresponded seq_len and 0 if position outside. Let's check out example >>> x.seq_len_mask Out: tensor([[1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0]]) There are 2, 8 and 5 valid tokens in lines. More way of seq_len_mask usage are in PaddedBatch docstring. We can recreate our x with modified content: x = PaddedBatch({k: v * x.seq_len_mask for k, v in x.payload.items()}, x.seq_lens) Now we can check x.payload and see features looks like real padded data: >>> x.payload['mcc_code'] Out: tensor([[8, 1, 0, 0, 0, 0, 0, 0], [5, 5, 9, 9, 4, 9, 3, 1], [4, 2, 2, 3, 3, 0, 0, 0]]) All invalid tokens are replaced with zeros. Generally, all layers respect PaddedBatch.seq_lens and no explicit zeroing of padded characters is required. Classes See docstrings for classes: ptls.data_load.padded_batch.PaddedBatch","title":"padded batch"},{"location":"data_load/padded_batch/#ptlsdata_loadpadded_batchpaddedbatch","text":"Input data is a raw feature formats. You can transform your transaction to correct format with ptls.data module. Common description or sequential data and used data formats are here Input data are covered in ptls.data_load.padded_batch.PaddedBatch class. We can create PaddedBatch object manually for demo and test purposes. x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) Here x contains three features. Two are categorical and one is numerical: mcc_code is categorical with dictionary_size=10 currency is categorical with dictionary_size=4 amount is numerical with mean=5 and std=4 x contains 5 sequences with maximum_length=12 . Real lengths of each sequence are [2, 8, 5] . We can access x content via PaddedBatch properties x.payload and x.seq_lens . Real data have sequences are padded with zeros. We can imitate it with x.seq_len_mask . It returns tensor with 1 if a position inside corresponded seq_len and 0 if position outside. Let's check out example >>> x.seq_len_mask Out: tensor([[1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0]]) There are 2, 8 and 5 valid tokens in lines. More way of seq_len_mask usage are in PaddedBatch docstring. We can recreate our x with modified content: x = PaddedBatch({k: v * x.seq_len_mask for k, v in x.payload.items()}, x.seq_lens) Now we can check x.payload and see features looks like real padded data: >>> x.payload['mcc_code'] Out: tensor([[8, 1, 0, 0, 0, 0, 0, 0], [5, 5, 9, 9, 4, 9, 3, 1], [4, 2, 2, 3, 3, 0, 0, 0]]) All invalid tokens are replaced with zeros. Generally, all layers respect PaddedBatch.seq_lens and no explicit zeroing of padded characters is required.","title":"ptls.data_load.padded_batch.PaddedBatch"},{"location":"data_load/padded_batch/#classes","text":"See docstrings for classes: ptls.data_load.padded_batch.PaddedBatch","title":"Classes"},{"location":"frames/bert/","text":"Bidirectional Encoder Representations from Transformers (BERT) Transformer based models pretrained with unsupervised task are state-of-the-art in NLP. We implement them for sequential data. Pretrain tasks are implemented: Replaced Token Detection (RTD) from ELECTRA Next Sequence Prediction (NSP) from BERT Sequences Order Prediction (SOP) from ALBERT Masked Language Model (MLM) from ROBERTA All of these tasks learn internal structure of the data and use it to make representation. NSP , RTD learn: 'global' representation of sequence embedding for each transaction is an internal state of seq_encoder embedding for all sequence is an output of seq_encoder RTD learn: 'local' representation of sequence embedding for each transaction is an internal state of seq_encoder embedding for all sequence available but aren't learned MLM learn: 'local' representation of sequence embedding for each transaction from trx_encoder pretrained MLM transformer as seq_encoder , CLS token aren't learned MLM ptls.frames.bert.MLMPretrainModule is a lightning module. ptls.frames.bert.MlmDataset , ptls.frames.bert.MlmIterableDataset is a compatible datasets. ptls.frames.bert.MlmIndexedDataset is also compatible with MLM. MlmDataset dataset sample one slice for one user. MlmIndexedDataset sample all possible slices for each user. MlmIndexedDataset index the data this because it hasn't iterable-style variant. RTD ptls.frames.bert.RtdModule is a lightning module. ptls.frames.bert.RtdDataset , ptls.frames.bert.RtdIterableDataset is a compatible datasets. SOP ptls.frames.bert.SopModule is a lightning module. ptls.frames.bert.SopDataset , ptls.frames.bert.SopIterableDataset is a compatible datasets. Requires splitter from ptls.frames.coles.split_strategy NSP ptls.frames.bert.NspModule is a lightning module. ptls.frames.bert.NspDataset , ptls.frames.bert.NspIterableDataset is a compatible datasets. Requires splitter from ptls.frames.coles.split_strategy Classes See docstrings for classes. ptls.frames.bert.MlmDataset ptls.frames.bert.MlmIterableDataset ptls.frames.bert.MlmIndexedDataset ptls.frames.bert.RtdDataset ptls.frames.bert.RtdIterableDataset ptls.frames.bert.SopDataset ptls.frames.bert.SopIterableDataset ptls.frames.bert.NspDataset ptls.frames.bert.NspIterableDataset ptls.frames.bert.MLMPretrainModule ptls.frames.bert.RtdModule ptls.frames.bert.SopModule ptls.frames.bert.NspModule","title":"bert"},{"location":"frames/bert/#bidirectional-encoder-representations-from-transformers-bert","text":"Transformer based models pretrained with unsupervised task are state-of-the-art in NLP. We implement them for sequential data. Pretrain tasks are implemented: Replaced Token Detection (RTD) from ELECTRA Next Sequence Prediction (NSP) from BERT Sequences Order Prediction (SOP) from ALBERT Masked Language Model (MLM) from ROBERTA All of these tasks learn internal structure of the data and use it to make representation. NSP , RTD learn: 'global' representation of sequence embedding for each transaction is an internal state of seq_encoder embedding for all sequence is an output of seq_encoder RTD learn: 'local' representation of sequence embedding for each transaction is an internal state of seq_encoder embedding for all sequence available but aren't learned MLM learn: 'local' representation of sequence embedding for each transaction from trx_encoder pretrained MLM transformer as seq_encoder , CLS token aren't learned","title":"Bidirectional Encoder Representations from Transformers (BERT)"},{"location":"frames/bert/#mlm","text":"ptls.frames.bert.MLMPretrainModule is a lightning module. ptls.frames.bert.MlmDataset , ptls.frames.bert.MlmIterableDataset is a compatible datasets. ptls.frames.bert.MlmIndexedDataset is also compatible with MLM. MlmDataset dataset sample one slice for one user. MlmIndexedDataset sample all possible slices for each user. MlmIndexedDataset index the data this because it hasn't iterable-style variant.","title":"MLM"},{"location":"frames/bert/#rtd","text":"ptls.frames.bert.RtdModule is a lightning module. ptls.frames.bert.RtdDataset , ptls.frames.bert.RtdIterableDataset is a compatible datasets.","title":"RTD"},{"location":"frames/bert/#sop","text":"ptls.frames.bert.SopModule is a lightning module. ptls.frames.bert.SopDataset , ptls.frames.bert.SopIterableDataset is a compatible datasets. Requires splitter from ptls.frames.coles.split_strategy","title":"SOP"},{"location":"frames/bert/#nsp","text":"ptls.frames.bert.NspModule is a lightning module. ptls.frames.bert.NspDataset , ptls.frames.bert.NspIterableDataset is a compatible datasets. Requires splitter from ptls.frames.coles.split_strategy","title":"NSP"},{"location":"frames/bert/#classes","text":"See docstrings for classes. ptls.frames.bert.MlmDataset ptls.frames.bert.MlmIterableDataset ptls.frames.bert.MlmIndexedDataset ptls.frames.bert.RtdDataset ptls.frames.bert.RtdIterableDataset ptls.frames.bert.SopDataset ptls.frames.bert.SopIterableDataset ptls.frames.bert.NspDataset ptls.frames.bert.NspIterableDataset ptls.frames.bert.MLMPretrainModule ptls.frames.bert.RtdModule ptls.frames.bert.SopModule ptls.frames.bert.NspModule","title":"Classes"},{"location":"frames/coles/","text":"Contrastive Learning for Event Sequences (CoLES) Original paper: CoLES: Contrastive Learning for Event Sequences with Self-Supervision CoLES is a framework that learn neural network to compress sequential data into a single embedding. Imagine a credit card transaction history that can be an example of user behavioral. Each user have his own behavioral patterns which are projected to his transaction history. Repeatability of behavioral patterns lead to repeatability in transaction history. CoLES exploit repeatability of patterns to make embedding. It samples a few subsequences from original sequence and calculates an embeddings for each of them. Embeddings are assigned to his user. Subsequences represent the same user and contain the same behavioral patterns. CoLES catch these patterns by making closer users embeddings. It also tries to distance different users embeddings. Subsequences represent also original sequence, and the similarity of behavioral patterns allows the similarity of embeddings for original sequence and his subsequence. CoLES learn: more 'global' representation of sequence embedding for each transaction is an internal state of seq_encoder embedding for all sequence is an output of seq_encoder CoLESModule ptls.frames.coles.CoLESModule is a LightningModule with CoLES framework. It should be parametrized by head and loss . Usually, loss requires a definition of sampling_strategy . CoLES datamodule requires a split_strategy . Combination of these parameters provides a variety of training methods. Example: seq_encoder = ... coles_module = CoLESModule( seq_encoder=seq_encoder, head=Head(use_norm_encoder=True), loss=ContrastiveLoss( margin=0.5, sampling_strategy=HardNegativePairSelector(neg_count=5), ) ) ColesDataset and split strategies Use ptls.frames.coles.ColesDataset or ptls.frames.coles.ColesIterableDataset . They are parametrised with splitter from ptls.frames.coles.split_strategy Coles losses and sampling strategies Use classes from: ptls.frames.coles.losses ptls.frames.coles.sampling_strategies Head selection Use ptls.nn.Head . Default head has only l2-norm layer ( Head(use_norm_encoder=True) ). Head with MLP inside realise projection head concept from SimCLR. Classes See docstrings for classes. ptls.frames.coles.ColesDataset ptls.frames.coles.ColesIterableDataset ptls.frames.coles.CoLESModule ptls.frames.coles.split_strategy ptls.frames.coles.losses ptls.frames.coles.sampling_strategies","title":"coles"},{"location":"frames/coles/#contrastive-learning-for-event-sequences-coles","text":"Original paper: CoLES: Contrastive Learning for Event Sequences with Self-Supervision CoLES is a framework that learn neural network to compress sequential data into a single embedding. Imagine a credit card transaction history that can be an example of user behavioral. Each user have his own behavioral patterns which are projected to his transaction history. Repeatability of behavioral patterns lead to repeatability in transaction history. CoLES exploit repeatability of patterns to make embedding. It samples a few subsequences from original sequence and calculates an embeddings for each of them. Embeddings are assigned to his user. Subsequences represent the same user and contain the same behavioral patterns. CoLES catch these patterns by making closer users embeddings. It also tries to distance different users embeddings. Subsequences represent also original sequence, and the similarity of behavioral patterns allows the similarity of embeddings for original sequence and his subsequence. CoLES learn: more 'global' representation of sequence embedding for each transaction is an internal state of seq_encoder embedding for all sequence is an output of seq_encoder","title":"Contrastive Learning for Event Sequences (CoLES)"},{"location":"frames/coles/#colesmodule","text":"ptls.frames.coles.CoLESModule is a LightningModule with CoLES framework. It should be parametrized by head and loss . Usually, loss requires a definition of sampling_strategy . CoLES datamodule requires a split_strategy . Combination of these parameters provides a variety of training methods. Example: seq_encoder = ... coles_module = CoLESModule( seq_encoder=seq_encoder, head=Head(use_norm_encoder=True), loss=ContrastiveLoss( margin=0.5, sampling_strategy=HardNegativePairSelector(neg_count=5), ) )","title":"CoLESModule"},{"location":"frames/coles/#colesdataset-and-split-strategies","text":"Use ptls.frames.coles.ColesDataset or ptls.frames.coles.ColesIterableDataset . They are parametrised with splitter from ptls.frames.coles.split_strategy","title":"ColesDataset and split strategies"},{"location":"frames/coles/#coles-losses-and-sampling-strategies","text":"Use classes from: ptls.frames.coles.losses ptls.frames.coles.sampling_strategies","title":"Coles losses and sampling strategies"},{"location":"frames/coles/#head-selection","text":"Use ptls.nn.Head . Default head has only l2-norm layer ( Head(use_norm_encoder=True) ). Head with MLP inside realise projection head concept from SimCLR.","title":"Head selection"},{"location":"frames/coles/#classes","text":"See docstrings for classes. ptls.frames.coles.ColesDataset ptls.frames.coles.ColesIterableDataset ptls.frames.coles.CoLESModule ptls.frames.coles.split_strategy ptls.frames.coles.losses ptls.frames.coles.sampling_strategies","title":"Classes"},{"location":"frames/common_usage/","text":"ptls.frames usage frames means frameworks. They are collects a popular technics to train a models. Each framework is a LightningModule . It means that you can train it with pytorch_lightning.Trainer . Frameworks consume data in a special format, so a LightningDataModule required. So there are three pytorch_lightning entities a required: model data trainer Trainer is a pytorch_lightning.Trainer . It automates training process. You can read its description here . We make a special torch.nn.Dataset implementation for each framework. All of them: support map and iterable version. You can use any of them. More info about have collate_fn for batch collection consume map or iterable input as dict of feature arrays compatible with ptls.frames.PtlsDataModule Model is usually seq_encoder with head optional. We provide a model to framework assigned LightningModule . Example This example is for CoLES framework. You can try an others with the same way. See module list in ptls.frames submodules. Check docstring for precise parameter tuning. Data generation We make a small test dataset. In real life you can use a many ways to load a data. See ptls.data_load . import torch # Makes 1000 samples with `mcc_code` and `amount` features and seq_len randomly sampled in range (100, 200) dataset = [{ 'mcc_code': torch.randint(1, 10, (seq_len,)), 'amount': torch.randn(seq_len), 'event_time': torch.arange(seq_len), # shows order between transactions } for seq_len in torch.randint(100, 200, (1000,))] from sklearn.model_selection import train_test_split # split 10% for validation train_data, valid_data = train_test_split(dataset, test_size=0.1) We can use an others sources for train and valid data. DataModule creation As we choose CoLES we should use ptls.frames.coles.ColesDataset for map style or ptls.frames.coles.ColesIterableDataset for iterable . Our demo data is in memory, so we can use both map or iterable . map style seems better because it provides better shuffle. If data is iterable like ptls.data_load.parquet_dataset.ParquetDataset we can't use map style until we read it to list . from ptls.frames.coles import ColesDataset from ptls.frames.coles.split_strategy import SampleSlices splitter=SampleSlices(split_count=5, cnt_min=10, cnt_max=20) train_dataset = ColesDataset(data=train_data, splitter=splitter) valid_dataset = ColesDataset(data=valid_data, splitter=splitter) Created datasets returns 5 subsample with length in range (10, 20) for each user. Now you need to create a dataloader that will collect batches. There are two ways to do this. Manual: train_dataloader = torch.utils.data.DataLoader( dataset=train_dataset, collate_fn=train_dataset.collate_fn, # collate_fn from dataset shuffle=True, num_workers=4, batch_size=32, ) valid_dataloader = torch.utils.data.DataLoader( dataset=valid_dataset, collate_fn=valid_dataset.collate_fn, # collate_fn from dataset shuffle=False, num_workers=0, batch_size=32, ) With datamodule: from ptls.frames import PtlsDataModule datamodule = PtlsDataModule( train_data=train_dataset, train_batch_size=32, train_num_workers=4, valid_data=valid_dataset, valid_num_workers=0, ) Model creation We have to create seq_cncoder that transform sequences to embedding and create CoLESModule that will train seq_cncoder . import torch.optim from functools import partial from ptls.nn import TrxEncoder, RnnSeqEncoder from ptls.frames.coles import CoLESModule seq_encoder = RnnSeqEncoder( trx_encoder=TrxEncoder( embeddings={'mcc_code': {'in': 10, 'out': 4}}, numeric_values={'amount': 'identity'}, ), hidden_size=16, # this is final embedding size ) coles_module = CoLESModule( seq_encoder=seq_encoder, optimizer_partial=partial(torch.optim.Adam, lr=0.001), lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=1, gamma=0.9), ) Training Everything is ready for training. Let's create a Trainer . import pytorch_lightning as pl trainer = pl.Trainer(gpus=1, max_epochs=50) There are many options for pytorch_lightning.Trainer check docstring. We force trainer to use one gpu with setting gpus=1 . If you haven't gpu keep gpus=None . Trainer will train our model until 50 epochs reached. Depending on the method of creating dataloaders, the learning interface changes slightly. With dataloaders: trainer.fit(coles_module, train_dataloader, valid_dataloader) With datamodule: trainer.fit(coles_module, datamodule) Result will be the same. Now coles_module with seq_encoder are trained. Inference This demo shows how to make embedding with pretrained seq_encoder . pytorch_lightning.Trainer have predict method that calls seq_encoder.forward . predict requires LightningModule but seq_encoder is torch.nn.Module . We should cover seq_encoder to LightningModule . We can use CoLESModule or any other module if available. In this example we can use coles_module object. Sometimes we have only seq_encoder , e.g. loaded from disk. CoLESModule have a little overhead. There are head, loss and metrics inside. Other way is using lightweight ptls.frames.supervised.SequenceToTarget module. It can run inference with only seq_encoder . import torch import pytorch_lightning as pl from ptls.frames.supervised import SequenceToTarget from ptls.data_load.datasets.dataloaders import inference_data_loader inference_dataloader = inference_data_loader(dataset, num_workers=4, batch_size=256) model = SequenceToTarget(seq_encoder) trainer = pl.Trainer(gpus=1) embeddings = torch.vstack(trainer.predict(model, inference_dataloader)) assert embeddings.size() == (1000, 16) Final shape is depends on: dataset size, we have 1000 samples in out dataset. seq_encoder.embedding_size , we set hidden_size=16 during RnnSeqEncoder creation. Next steps Now you can try to change hyperparameters of ColesDataset , CoLESModule and Trainer . Or try an others frameworks from ptls.frames .","title":"common usage pattern"},{"location":"frames/common_usage/#ptlsframes-usage","text":"frames means frameworks. They are collects a popular technics to train a models. Each framework is a LightningModule . It means that you can train it with pytorch_lightning.Trainer . Frameworks consume data in a special format, so a LightningDataModule required. So there are three pytorch_lightning entities a required: model data trainer Trainer is a pytorch_lightning.Trainer . It automates training process. You can read its description here . We make a special torch.nn.Dataset implementation for each framework. All of them: support map and iterable version. You can use any of them. More info about have collate_fn for batch collection consume map or iterable input as dict of feature arrays compatible with ptls.frames.PtlsDataModule Model is usually seq_encoder with head optional. We provide a model to framework assigned LightningModule .","title":"ptls.frames usage"},{"location":"frames/common_usage/#example","text":"This example is for CoLES framework. You can try an others with the same way. See module list in ptls.frames submodules. Check docstring for precise parameter tuning.","title":"Example"},{"location":"frames/common_usage/#data-generation","text":"We make a small test dataset. In real life you can use a many ways to load a data. See ptls.data_load . import torch # Makes 1000 samples with `mcc_code` and `amount` features and seq_len randomly sampled in range (100, 200) dataset = [{ 'mcc_code': torch.randint(1, 10, (seq_len,)), 'amount': torch.randn(seq_len), 'event_time': torch.arange(seq_len), # shows order between transactions } for seq_len in torch.randint(100, 200, (1000,))] from sklearn.model_selection import train_test_split # split 10% for validation train_data, valid_data = train_test_split(dataset, test_size=0.1) We can use an others sources for train and valid data.","title":"Data generation"},{"location":"frames/common_usage/#datamodule-creation","text":"As we choose CoLES we should use ptls.frames.coles.ColesDataset for map style or ptls.frames.coles.ColesIterableDataset for iterable . Our demo data is in memory, so we can use both map or iterable . map style seems better because it provides better shuffle. If data is iterable like ptls.data_load.parquet_dataset.ParquetDataset we can't use map style until we read it to list . from ptls.frames.coles import ColesDataset from ptls.frames.coles.split_strategy import SampleSlices splitter=SampleSlices(split_count=5, cnt_min=10, cnt_max=20) train_dataset = ColesDataset(data=train_data, splitter=splitter) valid_dataset = ColesDataset(data=valid_data, splitter=splitter) Created datasets returns 5 subsample with length in range (10, 20) for each user. Now you need to create a dataloader that will collect batches. There are two ways to do this. Manual: train_dataloader = torch.utils.data.DataLoader( dataset=train_dataset, collate_fn=train_dataset.collate_fn, # collate_fn from dataset shuffle=True, num_workers=4, batch_size=32, ) valid_dataloader = torch.utils.data.DataLoader( dataset=valid_dataset, collate_fn=valid_dataset.collate_fn, # collate_fn from dataset shuffle=False, num_workers=0, batch_size=32, ) With datamodule: from ptls.frames import PtlsDataModule datamodule = PtlsDataModule( train_data=train_dataset, train_batch_size=32, train_num_workers=4, valid_data=valid_dataset, valid_num_workers=0, )","title":"DataModule creation"},{"location":"frames/common_usage/#model-creation","text":"We have to create seq_cncoder that transform sequences to embedding and create CoLESModule that will train seq_cncoder . import torch.optim from functools import partial from ptls.nn import TrxEncoder, RnnSeqEncoder from ptls.frames.coles import CoLESModule seq_encoder = RnnSeqEncoder( trx_encoder=TrxEncoder( embeddings={'mcc_code': {'in': 10, 'out': 4}}, numeric_values={'amount': 'identity'}, ), hidden_size=16, # this is final embedding size ) coles_module = CoLESModule( seq_encoder=seq_encoder, optimizer_partial=partial(torch.optim.Adam, lr=0.001), lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=1, gamma=0.9), )","title":"Model creation"},{"location":"frames/common_usage/#training","text":"Everything is ready for training. Let's create a Trainer . import pytorch_lightning as pl trainer = pl.Trainer(gpus=1, max_epochs=50) There are many options for pytorch_lightning.Trainer check docstring. We force trainer to use one gpu with setting gpus=1 . If you haven't gpu keep gpus=None . Trainer will train our model until 50 epochs reached. Depending on the method of creating dataloaders, the learning interface changes slightly. With dataloaders: trainer.fit(coles_module, train_dataloader, valid_dataloader) With datamodule: trainer.fit(coles_module, datamodule) Result will be the same. Now coles_module with seq_encoder are trained.","title":"Training"},{"location":"frames/common_usage/#inference","text":"This demo shows how to make embedding with pretrained seq_encoder . pytorch_lightning.Trainer have predict method that calls seq_encoder.forward . predict requires LightningModule but seq_encoder is torch.nn.Module . We should cover seq_encoder to LightningModule . We can use CoLESModule or any other module if available. In this example we can use coles_module object. Sometimes we have only seq_encoder , e.g. loaded from disk. CoLESModule have a little overhead. There are head, loss and metrics inside. Other way is using lightweight ptls.frames.supervised.SequenceToTarget module. It can run inference with only seq_encoder . import torch import pytorch_lightning as pl from ptls.frames.supervised import SequenceToTarget from ptls.data_load.datasets.dataloaders import inference_data_loader inference_dataloader = inference_data_loader(dataset, num_workers=4, batch_size=256) model = SequenceToTarget(seq_encoder) trainer = pl.Trainer(gpus=1) embeddings = torch.vstack(trainer.predict(model, inference_dataloader)) assert embeddings.size() == (1000, 16) Final shape is depends on: dataset size, we have 1000 samples in out dataset. seq_encoder.embedding_size , we set hidden_size=16 during RnnSeqEncoder creation.","title":"Inference"},{"location":"frames/common_usage/#next-steps","text":"Now you can try to change hyperparameters of ColesDataset , CoLESModule and Trainer . Or try an others frameworks from ptls.frames .","title":"Next steps"},{"location":"frames/cpc/","text":"Contrastive Predictive Coding (CPC) Original paper: Representation Learning with Contrastive Predictive Coding CPC is a framework that learn neural network to predict the future state of sequence. CPC splits original sequence into small parts. Smallest part is a one transaction. trx_encoder or seq_encoder make a representation for each small part. So the original transaction sequence turns into sequence of embeddings. CPC tries to predict a next embedding in sequence. It takes into account some history of embeddings. Loss is contrastive, it uses random negative samples to avoid a trivial solution. CPC learn: more 'local' representation of sequence embedding for each transaction is a z state for CpcModule embedding for small parts of sequence is a z state for CpcV2Module embedding for all sequence is c - context state of CPC encoder CpcModule ptls.frames.cpc.CpcModule and ptls.frames.cpc.CpcV2Module is a LightningModule with CPC framework. It should be parametrized by n_negatives and n_forward_steps parameters. CpcV2Module parametrized also by aggregator network. CPC V2 datamodule requires a split strategy. Example: seq_encoder = ... coles_module = CpcModule( seq_encoder=seq_encoder, loss=CPC_Loss( n_negatives=16, n_forward_steps=3, ) ) CpcDataset and split strategies Use ptls.frames.cpc.CpcDataset or ptls.frames.cpc.CpcIterableDataset with CpcModule . Use ptls.frames.cpc.CpcV2Dataset or ptls.frames.cpc.CpcV2IterableDataset with CpcV2Module . Take splitter from ptls.frames.coles.split_strategy which preserve order in samples. Like SampleSlices(is_sorted=True) Classes See docstrings for classes. ptls.frames.cpc.CpcDataset ptls.frames.cpc.CpcIterableDataset ptls.frames.cpc.CpcV2Dataset ptls.frames.cpc.CpcV2IterableDataset ptls.frames.cpc.CpcModule ptls.frames.cpc.CpcV2Module ptls.frames.coles.split_strategy","title":"cpc"},{"location":"frames/cpc/#contrastive-predictive-coding-cpc","text":"Original paper: Representation Learning with Contrastive Predictive Coding CPC is a framework that learn neural network to predict the future state of sequence. CPC splits original sequence into small parts. Smallest part is a one transaction. trx_encoder or seq_encoder make a representation for each small part. So the original transaction sequence turns into sequence of embeddings. CPC tries to predict a next embedding in sequence. It takes into account some history of embeddings. Loss is contrastive, it uses random negative samples to avoid a trivial solution. CPC learn: more 'local' representation of sequence embedding for each transaction is a z state for CpcModule embedding for small parts of sequence is a z state for CpcV2Module embedding for all sequence is c - context state of CPC encoder","title":"Contrastive Predictive Coding (CPC)"},{"location":"frames/cpc/#cpcmodule","text":"ptls.frames.cpc.CpcModule and ptls.frames.cpc.CpcV2Module is a LightningModule with CPC framework. It should be parametrized by n_negatives and n_forward_steps parameters. CpcV2Module parametrized also by aggregator network. CPC V2 datamodule requires a split strategy. Example: seq_encoder = ... coles_module = CpcModule( seq_encoder=seq_encoder, loss=CPC_Loss( n_negatives=16, n_forward_steps=3, ) )","title":"CpcModule"},{"location":"frames/cpc/#cpcdataset-and-split-strategies","text":"Use ptls.frames.cpc.CpcDataset or ptls.frames.cpc.CpcIterableDataset with CpcModule . Use ptls.frames.cpc.CpcV2Dataset or ptls.frames.cpc.CpcV2IterableDataset with CpcV2Module . Take splitter from ptls.frames.coles.split_strategy which preserve order in samples. Like SampleSlices(is_sorted=True)","title":"CpcDataset and split strategies"},{"location":"frames/cpc/#classes","text":"See docstrings for classes. ptls.frames.cpc.CpcDataset ptls.frames.cpc.CpcIterableDataset ptls.frames.cpc.CpcV2Dataset ptls.frames.cpc.CpcV2IterableDataset ptls.frames.cpc.CpcModule ptls.frames.cpc.CpcV2Module ptls.frames.coles.split_strategy","title":"Classes"},{"location":"frames/inference/","text":"Inference Simple inference example are in demo/coles-emb.ipynb . It uses pretrained ptls.trames.coles.CoLESModule . It's also possible ptls.frames.supervised.SequenceToTarget with any pretrained. Both modules call forward method of internal model. forward returns tensor with embeddings, scores or PaddedBatch . In addition to the model output, we need to know which user this output is assigned to. In simple case we get it from data. We iterate over data twice: first to get sequential features from the model, second to get user ids. We should also avoid of data shuffle and keep item order to get correct match between model output and ids. More complicated inference way are in demo/extended_inference.ipynb . ptls.frames.inference_module.InferenceModule used. InferenceModule.forward accept PaddedPatch input with any types of features. Sequential are used to get model output. Other are passed to forward output. In other words InferenceModule works with nearly raw data and update it with model prediction. Usually we don't need sequential features in output, they will be dropped with InferenceModule.drop_seq_features=True . Output transformed to pandas.DataFrame with InferenceModule.pandas_output=True . You can't use InferenceModule for train due to output format. InferenceModule can be used with any pretrained models.","title":"inference"},{"location":"frames/inference/#inference","text":"Simple inference example are in demo/coles-emb.ipynb . It uses pretrained ptls.trames.coles.CoLESModule . It's also possible ptls.frames.supervised.SequenceToTarget with any pretrained. Both modules call forward method of internal model. forward returns tensor with embeddings, scores or PaddedBatch . In addition to the model output, we need to know which user this output is assigned to. In simple case we get it from data. We iterate over data twice: first to get sequential features from the model, second to get user ids. We should also avoid of data shuffle and keep item order to get correct match between model output and ids. More complicated inference way are in demo/extended_inference.ipynb . ptls.frames.inference_module.InferenceModule used. InferenceModule.forward accept PaddedPatch input with any types of features. Sequential are used to get model output. Other are passed to forward output. In other words InferenceModule works with nearly raw data and update it with model prediction. Usually we don't need sequential features in output, they will be dropped with InferenceModule.drop_seq_features=True . Output transformed to pandas.DataFrame with InferenceModule.pandas_output=True . You can't use InferenceModule for train due to output format. InferenceModule can be used with any pretrained models.","title":"Inference"},{"location":"frames/supervised/","text":"Supervised learning ptls.frames.supervised.SeqToTargetDataset and ptls.frames.supervised.SequenceToTarget for supervised learning SeqToTargetDataset Works similar as other datasets described in common patterns Source data should have a scalar field with target value. Example: dataset = SeqToTargetDataset([{ 'mcc_code': torch.randint(1, 10, (seq_len,)), 'amount': torch.randn(seq_len), 'event_time': torch.arange(seq_len), # shows order between transactions 'target': target, } for seq_len, target in zip( torch.randint(100, 200, (4,)), [0, 0, 1, 1], )], target_col_name='target') dl = torch.utils.data.DataLoader(dataset, batch_size=10, collate_fn=dataset.collate_fn) x, y = next(iter(dl)) torch.testing.assert_close(y, torch.LongTensor([0, 0, 1, 1])) SequenceToTarget in supervised mode SequenceToTarget is a lightning module for supervised training. This module assumes a target for sequence. There can be a some types of supervised task, like classification of regression. SequenceToTarget parameters allows to fit this module to your task. SequenceToTarget requires seq_encoder , head , loss and metrics . Se en examples of usage in SequenceToTarget docstring. Layers from seq_encoder , head can be randomly initialized or pretrained. SequenceToTarget in inference mode You may just provide pretrained seq_encoder to SequenceToTarget and use trainer.predict to get embeddings from pretrained seq_encoder . Classes See docstrings for classes. ptls.frames.supervised.SeqToTargetDataset ptls.frames.supervised.SequenceToTarget","title":"supervised"},{"location":"frames/supervised/#supervised-learning","text":"ptls.frames.supervised.SeqToTargetDataset and ptls.frames.supervised.SequenceToTarget for supervised learning","title":"Supervised learning"},{"location":"frames/supervised/#seqtotargetdataset","text":"Works similar as other datasets described in common patterns Source data should have a scalar field with target value. Example: dataset = SeqToTargetDataset([{ 'mcc_code': torch.randint(1, 10, (seq_len,)), 'amount': torch.randn(seq_len), 'event_time': torch.arange(seq_len), # shows order between transactions 'target': target, } for seq_len, target in zip( torch.randint(100, 200, (4,)), [0, 0, 1, 1], )], target_col_name='target') dl = torch.utils.data.DataLoader(dataset, batch_size=10, collate_fn=dataset.collate_fn) x, y = next(iter(dl)) torch.testing.assert_close(y, torch.LongTensor([0, 0, 1, 1]))","title":"SeqToTargetDataset"},{"location":"frames/supervised/#sequencetotarget-in-supervised-mode","text":"SequenceToTarget is a lightning module for supervised training. This module assumes a target for sequence. There can be a some types of supervised task, like classification of regression. SequenceToTarget parameters allows to fit this module to your task. SequenceToTarget requires seq_encoder , head , loss and metrics . Se en examples of usage in SequenceToTarget docstring. Layers from seq_encoder , head can be randomly initialized or pretrained.","title":"SequenceToTarget in supervised mode"},{"location":"frames/supervised/#sequencetotarget-in-inference-mode","text":"You may just provide pretrained seq_encoder to SequenceToTarget and use trainer.predict to get embeddings from pretrained seq_encoder .","title":"SequenceToTarget in inference mode"},{"location":"frames/supervised/#classes","text":"See docstrings for classes. ptls.frames.supervised.SeqToTargetDataset ptls.frames.supervised.SequenceToTarget","title":"Classes"},{"location":"frames/vicreg/","text":"Training methods Using Vicreg Loss with Metric Learn training Define the model from dltranz.seq_encoder import SequenceEncoder from dltranz.models import Head from dltranz.lightning_modules.emb_module import EmbModule from dltranz.metric_learn.losses import VicregLoss vicreg_loss = VicregLoss(sim_coeff=10.0, std_coeff=10.0, cov_coeff=5.0) seq_encoder = SequenceEncoder( category_features=preprocessor.get_category_sizes(), numeric_features=[\"amount_rur\"], trx_embedding_noize=0.003 ) head = Head(input_size=seq_encoder.embedding_size, hidden_layers_sizes=[256], use_batch_norm=True) model = EmbModule(seq_encoder=seq_encoder, head=head, loss=vicreg_loss, lr=0.01, lr_scheduler_step_size=10, lr_scheduler_step_gamma=0.9025) Data module To use VicregLoss and BarlowTwinsLoss it is crucial to set split_count=2 from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule dm = EmbeddingTrainDataModule( dataset=train, pl_module=model, min_seq_len=25, seq_split_strategy='SampleSlices', category_names = model.seq_encoder.category_names, category_max_size = model.seq_encoder.category_max_size, split_count=2, split_cnt_min=25, split_cnt_max=200, train_num_workers=16, train_batch_size=256, valid_num_workers=16, valid_batch_size=256 ) Training import torch import pytorch_lightning as pl import logging trainer = pl.Trainer( max_epochs=150, gpus=1 if torch.cuda.is_available() else 0 ) trainer.fit(model, dm)","title":"vicreg"},{"location":"frames/vicreg/#training-methods","text":"","title":"Training methods"},{"location":"frames/vicreg/#using-vicreg-loss-with-metric-learn-training","text":"","title":"Using Vicreg Loss with Metric Learn training"},{"location":"frames/vicreg/#define-the-model","text":"from dltranz.seq_encoder import SequenceEncoder from dltranz.models import Head from dltranz.lightning_modules.emb_module import EmbModule from dltranz.metric_learn.losses import VicregLoss vicreg_loss = VicregLoss(sim_coeff=10.0, std_coeff=10.0, cov_coeff=5.0) seq_encoder = SequenceEncoder( category_features=preprocessor.get_category_sizes(), numeric_features=[\"amount_rur\"], trx_embedding_noize=0.003 ) head = Head(input_size=seq_encoder.embedding_size, hidden_layers_sizes=[256], use_batch_norm=True) model = EmbModule(seq_encoder=seq_encoder, head=head, loss=vicreg_loss, lr=0.01, lr_scheduler_step_size=10, lr_scheduler_step_gamma=0.9025)","title":"Define the model"},{"location":"frames/vicreg/#data-module","text":"To use VicregLoss and BarlowTwinsLoss it is crucial to set split_count=2 from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule dm = EmbeddingTrainDataModule( dataset=train, pl_module=model, min_seq_len=25, seq_split_strategy='SampleSlices', category_names = model.seq_encoder.category_names, category_max_size = model.seq_encoder.category_max_size, split_count=2, split_cnt_min=25, split_cnt_max=200, train_num_workers=16, train_batch_size=256, valid_num_workers=16, valid_batch_size=256 )","title":"Data module"},{"location":"frames/vicreg/#training","text":"import torch import pytorch_lightning as pl import logging trainer = pl.Trainer( max_epochs=150, gpus=1 if torch.cuda.is_available() else 0 ) trainer.fit(model, dm)","title":"Training"},{"location":"nn/head/","text":"ptls.nn.head All classes from ptls.nn.head also available in ptls.nn ptls.nn.Head Head is a composition layer. Content is controlled by parameters. Such scenarios are possible: Empty layer. Do nothing. Can replace a default head: Head() L2 norm for output embedding: Head(use_norm_encoder=True) Binary classification head: Head(objective='classification', input_size=m) Multiclass classification head: Head(objective='classification', input_size=m, num_classes=n) Multilayer binary classification head: Head(objective='classification', input_size=m, hidden_layers_sizes=[i, j]) Regression head: Head(objective='regression', input_size=m) Their combinations are also possible Classes See docstrings for classes. ptls.nn.Head","title":"head"},{"location":"nn/head/#ptlsnnhead","text":"All classes from ptls.nn.head also available in ptls.nn","title":"ptls.nn.head"},{"location":"nn/head/#ptlsnnhead_1","text":"Head is a composition layer. Content is controlled by parameters. Such scenarios are possible: Empty layer. Do nothing. Can replace a default head: Head() L2 norm for output embedding: Head(use_norm_encoder=True) Binary classification head: Head(objective='classification', input_size=m) Multiclass classification head: Head(objective='classification', input_size=m, num_classes=n) Multilayer binary classification head: Head(objective='classification', input_size=m, hidden_layers_sizes=[i, j]) Regression head: Head(objective='regression', input_size=m) Their combinations are also possible","title":"ptls.nn.Head"},{"location":"nn/head/#classes","text":"See docstrings for classes. ptls.nn.Head","title":"Classes"},{"location":"nn/pb/","text":"ptls.nn.pb All classes from ptls.nn.pb also available in ptls.nn All classes in this module support PaddedBatch as input and output. Many modules extend torch.nn classes. Inherited layers Some layers are inherited from the original classes with forward reimplement. Original forward process x.payload . Result are packed to PaddedBatch . x.seq_lens passed to output PaddedBatch . PB-layers keep original class behavioral. Example: x = PaddedBatch(torch.randn(4, 12, 8), torch.LongTensor([3, 12, 8])) model = PBLinear(8, 5) y = model(x) assert y.payload.size() == (4, 12, 5) help(PBLinear) PB-layers can be used with other layers in torch.nn.Sequential x = PaddedBatch(torch.randn(4, 12, 8), torch.LongTensor([3, 12, 8])) model = torch.nn.Sequential( PBLinear(8, 5), PBReLU(), PBLinear(5, 10), ) y = model(x) assert y.payload.size() == (4, 12, 10) Class mapping Pb layer Parent Layer PBLinear torch.nn.Linear PBLayerNorm torch.nn.LayerNorm PBReLU torch.nn.ReLU PBL2Norm ptls.nn.L2NormEncoder Classes See docstrings for classes. ptls.nn.PBLinear ptls.nn.PBLayerNorm ptls.nn.PBL2Norm ptls.nn.PBReLU ptls.nn.PBL2Norm","title":"pb"},{"location":"nn/pb/#ptlsnnpb","text":"All classes from ptls.nn.pb also available in ptls.nn All classes in this module support PaddedBatch as input and output. Many modules extend torch.nn classes.","title":"ptls.nn.pb"},{"location":"nn/pb/#inherited-layers","text":"Some layers are inherited from the original classes with forward reimplement. Original forward process x.payload . Result are packed to PaddedBatch . x.seq_lens passed to output PaddedBatch . PB-layers keep original class behavioral. Example: x = PaddedBatch(torch.randn(4, 12, 8), torch.LongTensor([3, 12, 8])) model = PBLinear(8, 5) y = model(x) assert y.payload.size() == (4, 12, 5) help(PBLinear) PB-layers can be used with other layers in torch.nn.Sequential x = PaddedBatch(torch.randn(4, 12, 8), torch.LongTensor([3, 12, 8])) model = torch.nn.Sequential( PBLinear(8, 5), PBReLU(), PBLinear(5, 10), ) y = model(x) assert y.payload.size() == (4, 12, 10)","title":"Inherited layers"},{"location":"nn/pb/#class-mapping","text":"Pb layer Parent Layer PBLinear torch.nn.Linear PBLayerNorm torch.nn.LayerNorm PBReLU torch.nn.ReLU PBL2Norm ptls.nn.L2NormEncoder","title":"Class mapping"},{"location":"nn/pb/#classes","text":"See docstrings for classes. ptls.nn.PBLinear ptls.nn.PBLayerNorm ptls.nn.PBL2Norm ptls.nn.PBReLU ptls.nn.PBL2Norm","title":"Classes"},{"location":"nn/seq_encoder/","text":"ptls.nn.seq_encoder All classes from ptls.nn.seq_encoder also available in ptls.nn ptls.nn.trx_encoder works with individual transaction. ptls.nn.seq_encoder takes into account sequential structure and the links between transactions. There are 2 types of seq encoders: - required embeddings as input - requires raw features as input Embeddings as input We implement ptls-api for torch and huggingface sequential layers: ptls.nn.RnnEncoder for torch.nn.GRU ptls.nn.TransformerEncoder for torch.nn.TransformerEncoder ptls.nn.LongformerEncoder for transformers.LongformerModel They expect vectorized input, which can be obtained with TrxEncoder . Output format controlled by is_reduce_sequence property. True means that sequence will be reduced to one single vector. It's last hidden state for RNN and CLS token output for transformer. False means than all hidden vectors for all transactions will be returned. Set this property based on your needs. It's possible to set it during encoder initialisation. It's possible to change it in runtime. Simple Example: x = PaddedBatch(torch.randn(10, 80, 4), torch.randint(40, 80, (10,))) seq_encoder = RnnEncoder(input_size=4, hidden_size=16) y = seq_encoder(x) assert y.payload.size() == (10, 80, 16) More complicated example: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) trx_encoder = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) seq_encoder = RnnEncoder(input_size=trx_encoder.output_size, hidden_size=16) z = trx_encoder(x) y = seq_encoder(z) # embeddings wor each transaction seq_encoder.is_reduce_sequence = True h = seq_encoder(z) # embeddings for sequences, aggregate all transactions in one embedding assert y.payload.size() == (3, 8, 16) assert h.size() == (3, 16) Usually seq_encoder used with preliminary trx_encoder . It's possible to pack them to torch.nn.Sequential . It's possible to add more layers between trx_encoder and seq_encoder (linear, normalisation, convolutions, ...). They should work with PaddedBatch. Examples will be presented later. Such layers also works after seq_encoder with is_reduce_sequence=False . Features as input As you can see TrxEncoder works with raw features and compatible with embedding seq encoder. We make a composition layers, which contains TrxEncoder and one SeqEncoder implementation. There are: ptls.nn.RnnSeqEncoder with RnnEncoder ptls.nn.TransformerSeqEncoder with TransformerEncoder ptls.nn.LongformerSeqEncoder with LongformerEncoder They work as simple Sequential(trx_encoder, seq_encoder) and support is_reduce_sequence property. The main advantage that you can simply create such encoder from config file using hydra instantiate tools. You can avoid of explicit set of seq_encoder.input_size , they will be taken from trx_encoder . Let's compare. Sequential-style: config = \"\"\" model: _target_: torch.nn.Sequential _args_: - _target_: ptls.nn.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity - _target_: ptls.nn.RnnEncoder input_size: 9 # depends on TrxEncoder output hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] SeqEncoder-style: config = \"\"\" model: _target_: ptls.nn.RnnSeqEncoder trx_encoder: _target_: ptls.nn.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] The second config are simpler. Both of configs make an identical model. You can check: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) y = model(x) AggFeatureSeqEncoder ptls.nn.AggFeatureSeqEncoder . It looks like seq_encoder. It take raw features at input and provide reduced representation at output. This encoder creates features, which are good for boosting model. This is a strong baseline for many tasks. AggFeatureSeqEncoder eat the same input as other seq_encoders, and it can easily be replaced by rnn of transformer seq encoder. It use gpu and works fast. It haven't parameters for learn. Possible pipeline: seq_encoder = AggFeatureSeqEncoder(...) agg_embeddings = trainer.predict(seq_encoder, dataloader) catboost_model.fit(agg_embeddings, target) We plain to split AggFeatureSeqEncoder into components which will be compatible with other ptls-layers. It will be possible to choose flexible between TrxEncoder with AggSeqEncoder and OheEncoder with RnnEncoder . Classes See docstrings for classes. Take trx embedding as input: ptls.nn.RnnEncoder ptls.nn.TransformerEncoder ptls.nn.LongformerEncoder Take raw features as input: ptls.nn.RnnSeqEncoder ptls.nn.TransformerSeqEncoder ptls.nn.LongformerSeqEncoder ptls.nn.AggFeatureSeqEncoder","title":"seq_encoder"},{"location":"nn/seq_encoder/#ptlsnnseq_encoder","text":"All classes from ptls.nn.seq_encoder also available in ptls.nn ptls.nn.trx_encoder works with individual transaction. ptls.nn.seq_encoder takes into account sequential structure and the links between transactions. There are 2 types of seq encoders: - required embeddings as input - requires raw features as input","title":"ptls.nn.seq_encoder"},{"location":"nn/seq_encoder/#embeddings-as-input","text":"We implement ptls-api for torch and huggingface sequential layers: ptls.nn.RnnEncoder for torch.nn.GRU ptls.nn.TransformerEncoder for torch.nn.TransformerEncoder ptls.nn.LongformerEncoder for transformers.LongformerModel They expect vectorized input, which can be obtained with TrxEncoder . Output format controlled by is_reduce_sequence property. True means that sequence will be reduced to one single vector. It's last hidden state for RNN and CLS token output for transformer. False means than all hidden vectors for all transactions will be returned. Set this property based on your needs. It's possible to set it during encoder initialisation. It's possible to change it in runtime. Simple Example: x = PaddedBatch(torch.randn(10, 80, 4), torch.randint(40, 80, (10,))) seq_encoder = RnnEncoder(input_size=4, hidden_size=16) y = seq_encoder(x) assert y.payload.size() == (10, 80, 16) More complicated example: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) trx_encoder = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) seq_encoder = RnnEncoder(input_size=trx_encoder.output_size, hidden_size=16) z = trx_encoder(x) y = seq_encoder(z) # embeddings wor each transaction seq_encoder.is_reduce_sequence = True h = seq_encoder(z) # embeddings for sequences, aggregate all transactions in one embedding assert y.payload.size() == (3, 8, 16) assert h.size() == (3, 16) Usually seq_encoder used with preliminary trx_encoder . It's possible to pack them to torch.nn.Sequential . It's possible to add more layers between trx_encoder and seq_encoder (linear, normalisation, convolutions, ...). They should work with PaddedBatch. Examples will be presented later. Such layers also works after seq_encoder with is_reduce_sequence=False .","title":"Embeddings as input"},{"location":"nn/seq_encoder/#features-as-input","text":"As you can see TrxEncoder works with raw features and compatible with embedding seq encoder. We make a composition layers, which contains TrxEncoder and one SeqEncoder implementation. There are: ptls.nn.RnnSeqEncoder with RnnEncoder ptls.nn.TransformerSeqEncoder with TransformerEncoder ptls.nn.LongformerSeqEncoder with LongformerEncoder They work as simple Sequential(trx_encoder, seq_encoder) and support is_reduce_sequence property. The main advantage that you can simply create such encoder from config file using hydra instantiate tools. You can avoid of explicit set of seq_encoder.input_size , they will be taken from trx_encoder . Let's compare. Sequential-style: config = \"\"\" model: _target_: torch.nn.Sequential _args_: - _target_: ptls.nn.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity - _target_: ptls.nn.RnnEncoder input_size: 9 # depends on TrxEncoder output hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] SeqEncoder-style: config = \"\"\" model: _target_: ptls.nn.RnnSeqEncoder trx_encoder: _target_: ptls.nn.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] The second config are simpler. Both of configs make an identical model. You can check: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) y = model(x)","title":"Features as input"},{"location":"nn/seq_encoder/#aggfeatureseqencoder","text":"ptls.nn.AggFeatureSeqEncoder . It looks like seq_encoder. It take raw features at input and provide reduced representation at output. This encoder creates features, which are good for boosting model. This is a strong baseline for many tasks. AggFeatureSeqEncoder eat the same input as other seq_encoders, and it can easily be replaced by rnn of transformer seq encoder. It use gpu and works fast. It haven't parameters for learn. Possible pipeline: seq_encoder = AggFeatureSeqEncoder(...) agg_embeddings = trainer.predict(seq_encoder, dataloader) catboost_model.fit(agg_embeddings, target) We plain to split AggFeatureSeqEncoder into components which will be compatible with other ptls-layers. It will be possible to choose flexible between TrxEncoder with AggSeqEncoder and OheEncoder with RnnEncoder .","title":"AggFeatureSeqEncoder"},{"location":"nn/seq_encoder/#classes","text":"See docstrings for classes. Take trx embedding as input: ptls.nn.RnnEncoder ptls.nn.TransformerEncoder ptls.nn.LongformerEncoder Take raw features as input: ptls.nn.RnnSeqEncoder ptls.nn.TransformerSeqEncoder ptls.nn.LongformerSeqEncoder ptls.nn.AggFeatureSeqEncoder","title":"Classes"},{"location":"nn/trx_encoder/","text":"ptls.trx_encoder All classes from ptls.nn.trx_encoder also available in ptls.nn ptls.nn.trx_encoder helps to make a representation for single transactions. ptls.nn.TrxEncoder Now we have an input data: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) And se can define a TrxEncoder model = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) We should provide feature description to TrxEncoder . Dictionary size and embedding size for categorical features. Scaler name for numerical features. identity means no rescaling. TrxEncoder concatenate all feature embeddings, sow output embedding size will be 6 + 2 + 1 . You may get output size from TrxEncoder with property: >>> model.output_size Out[]: 6 Let's transform our features to embeddings z = model(x) z is also PaddedBatch . z.seq_lens equals x.seq_lens . z.payload isn't dict, it's tensor of shape (B, T, H). In our example B, T = 3, 8 is input feature shape, H = 6 is output size of model. Now we can use other layers which consume transactional embeddings. Classes See docstrings for classes: ptls.nn.TrxEncoder","title":"trx_encoder"},{"location":"nn/trx_encoder/#ptlstrx_encoder","text":"All classes from ptls.nn.trx_encoder also available in ptls.nn ptls.nn.trx_encoder helps to make a representation for single transactions.","title":"ptls.trx_encoder"},{"location":"nn/trx_encoder/#ptlsnntrxencoder","text":"Now we have an input data: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) And se can define a TrxEncoder model = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) We should provide feature description to TrxEncoder . Dictionary size and embedding size for categorical features. Scaler name for numerical features. identity means no rescaling. TrxEncoder concatenate all feature embeddings, sow output embedding size will be 6 + 2 + 1 . You may get output size from TrxEncoder with property: >>> model.output_size Out[]: 6 Let's transform our features to embeddings z = model(x) z is also PaddedBatch . z.seq_lens equals x.seq_lens . z.payload isn't dict, it's tensor of shape (B, T, H). In our example B, T = 3, 8 is input feature shape, H = 6 is output size of model. Now we can use other layers which consume transactional embeddings.","title":"ptls.nn.TrxEncoder"},{"location":"nn/trx_encoder/#classes","text":"See docstrings for classes: ptls.nn.TrxEncoder","title":"Classes"}]}