{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to pytorch-lifestream docs Library content Here is a breaf overview of library with links to detail description. Some sections are in the process of description. Library modules: - ptls.data - all you need for prepare your data for neural network feeding. Link - ptls.data.preprocessing - common patters for feature transformation. Categorical encoding, datetime transformation, numerical feature preprocessing. - ptls.data.split_tools - convert data to ptls-data-format. Split by users and features. - ptls.data.datasets - torch.Dataset interface access to the data. - ptls.lightning_modules - propose tools for training your encoders with popular frameworks like CoLES, SimCLR, CPC, VICReg, ... Link TBD - Build your encoder with layers. Link TBD : - ptls.trx_encoder - layers which makes representation for single transactions. Link - ptls.seq_encoder - layers which works with sequences. There are cross transactional interaction (like convolution of self attention) and reduce sequence to single vector. Link - ptls.heads - composite layers for final embedding transformation (L2Norm, MLP, ResNet, FFT, ...). Link TBD - ptls.contrastive_learning - propose tools for contrastive learning tasks. There are losses, mining strategies and metrics. Link TBD How to guide Prepare your data . Use Pyspark in local or cluster mode for big dataset and Pandas for small. Split data into required parts (train, valid, test, ...). Transform features to compatible format using Pyspark or Pandas functions. You can use also ptls.data.preprocessing for common data transformation patterns. Split sequences to ptls-data format with ptls.data.split_tools . Save prepared data into Parquet format or keep it in memory ( Pickle also works). Use one of available ptls.data.datasets which provide data access. Choose framework for encoder train . There are both supervised of unsupervised in ptls.lightning_modules . Keep in mind that each framework requires his own batch format. Tools for batch collate can be found in the selected framework package. Build encoder . All parts are available in ptls.trx_encoder , ptls.seq_encoder , ptls.heads . You can also use pretrained layers. Train your encoder with selected framework. Some frameworks require certain options to be selected. It can be loss, metrics or something else. ptls.contrastive_learning contains all of them. Provide data with selected framework compatible dataloader. Check the progress on tensorboard. Tune hyperparameters if you need. Save trained encoder for future use. You can use it as single solution (e.g. get class label probabilities). It can be a pretrained part of other neural network. Use encoder in your project. Run predict for your data and get logits, probas, scores or embeddings. Use ptls.data and ptls.data.datasets tools to keep your data transformation and collect batches for inference. How to create your own components Your task may require some specific solution. You may create a new component for every library modules. There are links with description: - ptls.data - new data processing tools and datasets for new type of data. Link TBD - ptls.lightning_modules - new modules which train network for your problem. Link TBD - New layers for ptls.trx_encoder , ptls.seq_encoder , ptls.heads . Link TBD - ptls.contrastive_learning - New losses, mining strategies and metrics. Link TBD","title":"Home"},{"location":"#welcome-to-pytorch-lifestream-docs","text":"","title":"Welcome to pytorch-lifestream docs"},{"location":"#library-content","text":"Here is a breaf overview of library with links to detail description. Some sections are in the process of description. Library modules: - ptls.data - all you need for prepare your data for neural network feeding. Link - ptls.data.preprocessing - common patters for feature transformation. Categorical encoding, datetime transformation, numerical feature preprocessing. - ptls.data.split_tools - convert data to ptls-data-format. Split by users and features. - ptls.data.datasets - torch.Dataset interface access to the data. - ptls.lightning_modules - propose tools for training your encoders with popular frameworks like CoLES, SimCLR, CPC, VICReg, ... Link TBD - Build your encoder with layers. Link TBD : - ptls.trx_encoder - layers which makes representation for single transactions. Link - ptls.seq_encoder - layers which works with sequences. There are cross transactional interaction (like convolution of self attention) and reduce sequence to single vector. Link - ptls.heads - composite layers for final embedding transformation (L2Norm, MLP, ResNet, FFT, ...). Link TBD - ptls.contrastive_learning - propose tools for contrastive learning tasks. There are losses, mining strategies and metrics. Link TBD","title":"Library content"},{"location":"#how-to-guide","text":"Prepare your data . Use Pyspark in local or cluster mode for big dataset and Pandas for small. Split data into required parts (train, valid, test, ...). Transform features to compatible format using Pyspark or Pandas functions. You can use also ptls.data.preprocessing for common data transformation patterns. Split sequences to ptls-data format with ptls.data.split_tools . Save prepared data into Parquet format or keep it in memory ( Pickle also works). Use one of available ptls.data.datasets which provide data access. Choose framework for encoder train . There are both supervised of unsupervised in ptls.lightning_modules . Keep in mind that each framework requires his own batch format. Tools for batch collate can be found in the selected framework package. Build encoder . All parts are available in ptls.trx_encoder , ptls.seq_encoder , ptls.heads . You can also use pretrained layers. Train your encoder with selected framework. Some frameworks require certain options to be selected. It can be loss, metrics or something else. ptls.contrastive_learning contains all of them. Provide data with selected framework compatible dataloader. Check the progress on tensorboard. Tune hyperparameters if you need. Save trained encoder for future use. You can use it as single solution (e.g. get class label probabilities). It can be a pretrained part of other neural network. Use encoder in your project. Run predict for your data and get logits, probas, scores or embeddings. Use ptls.data and ptls.data.datasets tools to keep your data transformation and collect batches for inference.","title":"How to guide"},{"location":"#how-to-create-your-own-components","text":"Your task may require some specific solution. You may create a new component for every library modules. There are links with description: - ptls.data - new data processing tools and datasets for new type of data. Link TBD - ptls.lightning_modules - new modules which train network for your problem. Link TBD - New layers for ptls.trx_encoder , ptls.seq_encoder , ptls.heads . Link TBD - ptls.contrastive_learning - New losses, mining strategies and metrics. Link TBD","title":"How to create your own components"},{"location":"data_preparation/","text":"Data preparation Source data We address the problem of learning on discrete event sequences generated by real-world users. Raw table data Lifestream data can be presented as table where rows are events and columns are event attributes. Columns can be of the following data types: - user_id - id for collecting events in sequences. We assume that there are many users in the dataset and associated sequences of events. An event can only be linked to one user. - event_time - is timestamp, used for ordering events in sequence. It's possible extract date-time features from timestamp. If the timestamp is not available, you can use any data type that can define the order. - feature fields - describe a properties of events. Can be numerical, categorical or any type that can be converted to feature vector. Credit card transaction history is a example of lifestream data. client_id date_time mcc_code amount A0001 2021-03-01 12:00:00 6011 1000.00 A0001 2021-03-01 12:15:00 4814 12.05 A0001 2021-03-04 10:00:00 5411 2312.99 A0001 2021-03-04 10:00:00 5411 199.99 E0123 2021-02-05 13:10:00 6536 12300.00 E0123 2021-03-05 12:04:00 6536 12300.00 E0123 2021-04-05 11:22:00 6536 12300.00 In this example we can find two users (clients) with two sequences. First contains 4 events, second contains 3 events. We sort events by date_time for each user to assure correct event order. Each event (transaction) are described by categorical field mcc_code , numerical field amount , and time field date_time . These fields allow to distinguish events, vectorize them na use as a features. pytorch-lifeatream supports this format of data and provides the tools to process it throw the pipeline. Data can be pandas.DataFrame or pyspark.DataFrame . Data collected in lists Table data should be converted to format more convenient for neural network feeding. There are steps: 1. Feature field transformation: encoding categorical features, amount normalizing, missing values imputing. This works like sklearn fit-transform preprocessors. 2. Splitting all events by user_id and sort events by event_time . We transfer flat table with events to set of users with event collections. 3. Split events by feature fields. Features are stored as 1d-arrays. Sequence orders are kept. Previous example with can be presented as (feature transformation missed for visibility): [ { client_id: 'A0001', date_time: [2021-03-01 12:00:00, 2021-03-01 12:15:00, 2021-03-04 10:00:00, 2021-03-04 10:00:00], mcc_code: [6011, 4814, 5411, 5411], amount: [1000.00, 12.05, 2312.99, 199.99], }, { client_id: 'E0123', date_time: [2021-02-05 13:10:00, 2021-03-05 12:04:00, 2021-04-05 11:22:00], mcc_code: [6536, 6536, 6536], amount: [12300.00, 12300.00, 12300.00], }, ] This is a main input data format in pytorch-lifeatream . Supported: - convert from raw table to collected lists both for pandas.DataFrame and pyspark.DataFrame - fast end effective storage in parquet format - compatible torch.Dataset and torch.Dataloader - in-memory augmentations and transformations Dataset pytorch-lifeatream provide multiple torch.Dataset implementations. Dataset item present single user information and can be a combination of: - record - is a dictionary where kees are feature names and values are 1d-tensors with feature sequences. Similar as data collected in lists. - id - how to identify a sequence - target - target value for supervised learning Code example: dataset = SomeDataset(params) X, y = dataset[0] DataLoader The main feature of putorch-lifestream dataloader is customized collate_fn , provided to torch.DataLoader class. collate_fn collects single records of dictionaries to batch. Usually collate_fn pad and pack sequences into 2d tensors with size BxT , where B - is sample num and T is max sequence length. Each feature packed separately. Output is PaddedBatch type which collect together packed sequences and lengths. PaddedBatch compatible with all putorch-lifestream modules. Input and output example: # input batch = [ {'cat1': [0, 1, 2, 3], 'amnt': [10, 20, 10, 10]}, {'cat1': [3, 1], 'amnt': [13, 6]}, {'cat1': [1, 2, 3], 'amnt': [10, 4, 10]}, ] batch = PaddedBatch( payload = { 'cat1': [ [0, 1, 2, 3], [3, 1, 0, 0], [1, 2, 3, 0], ], 'amnt': [ [10, 20, 10, 10], [13, 6, 0, 0], [10, 4, 10, 0], ] }, seq_len = [4, 2, 3] )","title":"Data Preparation"},{"location":"data_preparation/#data-preparation","text":"","title":"Data preparation"},{"location":"data_preparation/#source-data","text":"We address the problem of learning on discrete event sequences generated by real-world users.","title":"Source data"},{"location":"data_preparation/#raw-table-data","text":"Lifestream data can be presented as table where rows are events and columns are event attributes. Columns can be of the following data types: - user_id - id for collecting events in sequences. We assume that there are many users in the dataset and associated sequences of events. An event can only be linked to one user. - event_time - is timestamp, used for ordering events in sequence. It's possible extract date-time features from timestamp. If the timestamp is not available, you can use any data type that can define the order. - feature fields - describe a properties of events. Can be numerical, categorical or any type that can be converted to feature vector. Credit card transaction history is a example of lifestream data. client_id date_time mcc_code amount A0001 2021-03-01 12:00:00 6011 1000.00 A0001 2021-03-01 12:15:00 4814 12.05 A0001 2021-03-04 10:00:00 5411 2312.99 A0001 2021-03-04 10:00:00 5411 199.99 E0123 2021-02-05 13:10:00 6536 12300.00 E0123 2021-03-05 12:04:00 6536 12300.00 E0123 2021-04-05 11:22:00 6536 12300.00 In this example we can find two users (clients) with two sequences. First contains 4 events, second contains 3 events. We sort events by date_time for each user to assure correct event order. Each event (transaction) are described by categorical field mcc_code , numerical field amount , and time field date_time . These fields allow to distinguish events, vectorize them na use as a features. pytorch-lifeatream supports this format of data and provides the tools to process it throw the pipeline. Data can be pandas.DataFrame or pyspark.DataFrame .","title":"Raw table data"},{"location":"data_preparation/#data-collected-in-lists","text":"Table data should be converted to format more convenient for neural network feeding. There are steps: 1. Feature field transformation: encoding categorical features, amount normalizing, missing values imputing. This works like sklearn fit-transform preprocessors. 2. Splitting all events by user_id and sort events by event_time . We transfer flat table with events to set of users with event collections. 3. Split events by feature fields. Features are stored as 1d-arrays. Sequence orders are kept. Previous example with can be presented as (feature transformation missed for visibility): [ { client_id: 'A0001', date_time: [2021-03-01 12:00:00, 2021-03-01 12:15:00, 2021-03-04 10:00:00, 2021-03-04 10:00:00], mcc_code: [6011, 4814, 5411, 5411], amount: [1000.00, 12.05, 2312.99, 199.99], }, { client_id: 'E0123', date_time: [2021-02-05 13:10:00, 2021-03-05 12:04:00, 2021-04-05 11:22:00], mcc_code: [6536, 6536, 6536], amount: [12300.00, 12300.00, 12300.00], }, ] This is a main input data format in pytorch-lifeatream . Supported: - convert from raw table to collected lists both for pandas.DataFrame and pyspark.DataFrame - fast end effective storage in parquet format - compatible torch.Dataset and torch.Dataloader - in-memory augmentations and transformations","title":"Data collected in lists"},{"location":"data_preparation/#dataset","text":"pytorch-lifeatream provide multiple torch.Dataset implementations. Dataset item present single user information and can be a combination of: - record - is a dictionary where kees are feature names and values are 1d-tensors with feature sequences. Similar as data collected in lists. - id - how to identify a sequence - target - target value for supervised learning Code example: dataset = SomeDataset(params) X, y = dataset[0]","title":"Dataset"},{"location":"data_preparation/#dataloader","text":"The main feature of putorch-lifestream dataloader is customized collate_fn , provided to torch.DataLoader class. collate_fn collects single records of dictionaries to batch. Usually collate_fn pad and pack sequences into 2d tensors with size BxT , where B - is sample num and T is max sequence length. Each feature packed separately. Output is PaddedBatch type which collect together packed sequences and lengths. PaddedBatch compatible with all putorch-lifestream modules. Input and output example: # input batch = [ {'cat1': [0, 1, 2, 3], 'amnt': [10, 20, 10, 10]}, {'cat1': [3, 1], 'amnt': [13, 6]}, {'cat1': [1, 2, 3], 'amnt': [10, 4, 10]}, ] batch = PaddedBatch( payload = { 'cat1': [ [0, 1, 2, 3], [3, 1, 0, 0], [1, 2, 3, 0], ], 'amnt': [ [10, 20, 10, 10], [13, 6, 0, 0], [10, 4, 10, 0], ] }, seq_len = [4, 2, 3] )","title":"DataLoader"},{"location":"embeddings_with_other_losses/","text":"Using Vicreg Loss with Metric Learn training Define the model from dltranz.seq_encoder import SequenceEncoder from dltranz.models import Head from dltranz.lightning_modules.emb_module import EmbModule from dltranz.metric_learn.losses import VicregLoss vicreg_loss = VicregLoss(sim_coeff=10.0, std_coeff=10.0, cov_coeff=5.0) seq_encoder = SequenceEncoder( category_features=preprocessor.get_category_sizes(), numeric_features=[\"amount_rur\"], trx_embedding_noize=0.003 ) head = Head(input_size=seq_encoder.embedding_size, hidden_layers_sizes=[256], use_batch_norm=True) model = EmbModule(seq_encoder=seq_encoder, head=head, loss=vicreg_loss, lr=0.01, lr_scheduler_step_size=10, lr_scheduler_step_gamma=0.9025) Data module To use VicregLoss and BarlowTwinsLoss it is crucial to set split_count=2 from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule dm = EmbeddingTrainDataModule( dataset=train, pl_module=model, min_seq_len=25, seq_split_strategy='SampleSlices', category_names = model.seq_encoder.category_names, category_max_size = model.seq_encoder.category_max_size, split_count=2, split_cnt_min=25, split_cnt_max=200, train_num_workers=16, train_batch_size=256, valid_num_workers=16, valid_batch_size=256 ) Training import torch import pytorch_lightning as pl import logging trainer = pl.Trainer( max_epochs=150, gpus=1 if torch.cuda.is_available() else 0 ) trainer.fit(model, dm)","title":"Losses"},{"location":"embeddings_with_other_losses/#using-vicreg-loss-with-metric-learn-training","text":"","title":"Using Vicreg Loss with Metric Learn training"},{"location":"embeddings_with_other_losses/#define-the-model","text":"from dltranz.seq_encoder import SequenceEncoder from dltranz.models import Head from dltranz.lightning_modules.emb_module import EmbModule from dltranz.metric_learn.losses import VicregLoss vicreg_loss = VicregLoss(sim_coeff=10.0, std_coeff=10.0, cov_coeff=5.0) seq_encoder = SequenceEncoder( category_features=preprocessor.get_category_sizes(), numeric_features=[\"amount_rur\"], trx_embedding_noize=0.003 ) head = Head(input_size=seq_encoder.embedding_size, hidden_layers_sizes=[256], use_batch_norm=True) model = EmbModule(seq_encoder=seq_encoder, head=head, loss=vicreg_loss, lr=0.01, lr_scheduler_step_size=10, lr_scheduler_step_gamma=0.9025)","title":"Define the model"},{"location":"embeddings_with_other_losses/#data-module","text":"To use VicregLoss and BarlowTwinsLoss it is crucial to set split_count=2 from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule dm = EmbeddingTrainDataModule( dataset=train, pl_module=model, min_seq_len=25, seq_split_strategy='SampleSlices', category_names = model.seq_encoder.category_names, category_max_size = model.seq_encoder.category_max_size, split_count=2, split_cnt_min=25, split_cnt_max=200, train_num_workers=16, train_batch_size=256, valid_num_workers=16, valid_batch_size=256 )","title":"Data module"},{"location":"embeddings_with_other_losses/#training","text":"import torch import pytorch_lightning as pl import logging trainer = pl.Trainer( max_epochs=150, gpus=1 if torch.cuda.is_available() else 0 ) trainer.fit(model, dm)","title":"Training"},{"location":"seq_encoder/","text":"ptls.seq_encoder usage Usage trx_encoder works with individual transaction. seq_encoder takes into account sequential structure and the links between transactions. There are 2 types of seq encoders: - required embeddings as input - requires raw features as input Embeddings as input We implement ptls-api for torch sequential layers: - ptls.seq_encoder.RnnEncoder for torch.nn.GRU - ptls.seq_encoder.TransformerEncoder for torch.nn.TransformerEncoder They expect vectorized input, which can be obtained with TrxEncoder . Output format controlled by is_reduce_sequence property. True means that sequence will be reduced to one single vector. It's last hidden state for RNN and CLS token output for transformer. False means than all hidden vectors for all transactions will be returned. Set this property based on your needs. It's possible to set it during encoder initialisation. It's possible to change it in runtime. Simple Example: x = PaddedBatch(torch.randn(10, 80, 4), torch.randint(40, 80, (10,))) seq_encoder = RnnEncoder(input_size=4, hidden_size=16) y = seq_encoder(x) assert y.payload.size() == (10, 80, 16) More complicated example: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) trx_encoder = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) seq_encoder = RnnEncoder(input_size=trx_encoder.output_size, hidden_size=16) z = trx_encoder(x) y = seq_encoder(z) # embeddings wor each transaction seq_encoder.is_reduce_sequence = True h = seq_encoder(z) # embeddings for sequences, aggregate all transactions in one embedding assert y.payload.size() == (3, 8, 16) assert h.size() == (3, 16) Usually seq_encoder used with preliminary trx_encoder . It's possible to pack them to torch.nn.Sequential . It's possible to add more layers between trx_encoder and seq_encoder (linear, normalisation, convolutions, ...). They should work with PaddedBatch. Examples will be presented later. Such layers also works after seq_encoder with is_reduce_sequence=False . Features as input As you can see TrxEncoder works with raw features and compatible with embedding seq encoder. We make a composition layers, which contains TrxEncoder and one SeqEncoder implementation. There are: - ptls.seq_encoder.RnnSeqEncoder with RnnEncoder - ptls.seq_encoder.TransformerSeqEncoder with TransformerEncoder They work as simple Sequential(trx_encoder, seq_encoder) and support is_reduce_sequence property. The main advantage that you can simply create such encoder from config file using hydra instantiate tools. You can avoid of explicit set of seq_encoder.input_size , they will be taken from trx_encoder . Let's compare. Sequential-style: config = \"\"\" model: _target_: torch.nn.Sequential _args_: - _target_: ptls.trx_encoder.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity - _target_: ptls.seq_encoder.RnnEncoder input_size: 9 # depends on TrxEncoder output hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] SeqEncoder-style: config = \"\"\" model: _target_: ptls.seq_encoder.RnnSeqEncoder trx_encoder: _target_: ptls.trx_encoder.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] The second config are simpler. Both of configs make an identical model. You can check: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) y = model(x) Also we have ptls.seq_encoder.AggFeatureSeqEncoder . It looks like seq_encoder. It take raw features at input and provide reduced representation at output. This encoder creates features, which are good for boosting model. This is a strong baseline for many tasks. AggFeatureSeqEncoder eat the same input as other seq_encoders, and it can easily be replaced by rnn of transformer seq encoder. It use gpu and works fast. It haven't parameters for learn. Possible pipeline: seq_encoder = AggFeatureSeqEncoder(...) agg_embeddings = trainer.predict(seq_encoder, dataloader) catboost_model.fit(agg_embeddings, target) We plain to split AggFeatureSeqEncoder into components which will be compatible with other ptls-layers. It will be possible to choose flexible between TrxEncoder with AggSeqEncoder and OheEncoder with RnnEncoder . Classes See docstrings for classes. Take trx embedding as input: - ptls.seq_encoder.RnnEncoder - ptls.seq_encoder.TransformerEncoder Take raw features as input: - ptls.seq_encoder.RnnSeqEncoder - ptls.seq_encoder.TransformerSeqEncoder - ptls.seq_encoder.AggFeatureSeqEncoder","title":"seq_encoder"},{"location":"seq_encoder/#ptlsseq_encoder-usage","text":"","title":"ptls.seq_encoder usage"},{"location":"seq_encoder/#usage","text":"trx_encoder works with individual transaction. seq_encoder takes into account sequential structure and the links between transactions. There are 2 types of seq encoders: - required embeddings as input - requires raw features as input","title":"Usage"},{"location":"seq_encoder/#embeddings-as-input","text":"We implement ptls-api for torch sequential layers: - ptls.seq_encoder.RnnEncoder for torch.nn.GRU - ptls.seq_encoder.TransformerEncoder for torch.nn.TransformerEncoder They expect vectorized input, which can be obtained with TrxEncoder . Output format controlled by is_reduce_sequence property. True means that sequence will be reduced to one single vector. It's last hidden state for RNN and CLS token output for transformer. False means than all hidden vectors for all transactions will be returned. Set this property based on your needs. It's possible to set it during encoder initialisation. It's possible to change it in runtime. Simple Example: x = PaddedBatch(torch.randn(10, 80, 4), torch.randint(40, 80, (10,))) seq_encoder = RnnEncoder(input_size=4, hidden_size=16) y = seq_encoder(x) assert y.payload.size() == (10, 80, 16) More complicated example: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) trx_encoder = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) seq_encoder = RnnEncoder(input_size=trx_encoder.output_size, hidden_size=16) z = trx_encoder(x) y = seq_encoder(z) # embeddings wor each transaction seq_encoder.is_reduce_sequence = True h = seq_encoder(z) # embeddings for sequences, aggregate all transactions in one embedding assert y.payload.size() == (3, 8, 16) assert h.size() == (3, 16) Usually seq_encoder used with preliminary trx_encoder . It's possible to pack them to torch.nn.Sequential . It's possible to add more layers between trx_encoder and seq_encoder (linear, normalisation, convolutions, ...). They should work with PaddedBatch. Examples will be presented later. Such layers also works after seq_encoder with is_reduce_sequence=False .","title":"Embeddings as input"},{"location":"seq_encoder/#features-as-input","text":"As you can see TrxEncoder works with raw features and compatible with embedding seq encoder. We make a composition layers, which contains TrxEncoder and one SeqEncoder implementation. There are: - ptls.seq_encoder.RnnSeqEncoder with RnnEncoder - ptls.seq_encoder.TransformerSeqEncoder with TransformerEncoder They work as simple Sequential(trx_encoder, seq_encoder) and support is_reduce_sequence property. The main advantage that you can simply create such encoder from config file using hydra instantiate tools. You can avoid of explicit set of seq_encoder.input_size , they will be taken from trx_encoder . Let's compare. Sequential-style: config = \"\"\" model: _target_: torch.nn.Sequential _args_: - _target_: ptls.trx_encoder.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity - _target_: ptls.seq_encoder.RnnEncoder input_size: 9 # depends on TrxEncoder output hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] SeqEncoder-style: config = \"\"\" model: _target_: ptls.seq_encoder.RnnSeqEncoder trx_encoder: _target_: ptls.trx_encoder.TrxEncoder embeddings: mcc_code: in: 10 out: 6 currency: in: 4 out: 2 numeric_values: amount: identity hidden_size: 24 \"\"\" model = hydra.utils.instantiate(OmegaConf.create(config))['model'] The second config are simpler. Both of configs make an identical model. You can check: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) y = model(x) Also we have ptls.seq_encoder.AggFeatureSeqEncoder . It looks like seq_encoder. It take raw features at input and provide reduced representation at output. This encoder creates features, which are good for boosting model. This is a strong baseline for many tasks. AggFeatureSeqEncoder eat the same input as other seq_encoders, and it can easily be replaced by rnn of transformer seq encoder. It use gpu and works fast. It haven't parameters for learn. Possible pipeline: seq_encoder = AggFeatureSeqEncoder(...) agg_embeddings = trainer.predict(seq_encoder, dataloader) catboost_model.fit(agg_embeddings, target) We plain to split AggFeatureSeqEncoder into components which will be compatible with other ptls-layers. It will be possible to choose flexible between TrxEncoder with AggSeqEncoder and OheEncoder with RnnEncoder .","title":"Features as input"},{"location":"seq_encoder/#classes","text":"See docstrings for classes. Take trx embedding as input: - ptls.seq_encoder.RnnEncoder - ptls.seq_encoder.TransformerEncoder Take raw features as input: - ptls.seq_encoder.RnnSeqEncoder - ptls.seq_encoder.TransformerSeqEncoder - ptls.seq_encoder.AggFeatureSeqEncoder","title":"Classes"},{"location":"trx_encoder/","text":"ptls.trx_encoder usage Usage ptls.trx_encoder helps to make a representation for single transactions. ptls.trx_encoder.PaddedBatch Input data is a raw feature formats. You can transform your transaction to correct format with ptls.data module. Some description about this process are here data_preparation.md Input data are covered in ptls.trx_encoder.PaddedBatch class. We can create PaddedBatch object manually for demo and test purposes. x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) Here x contains three features. Two are categorical and one is numerical: - mcc_code is categorical with dictionary_size=10 - currency is categorical with dictionary_size=4 - amount is numerical with mean=5 and std=4 x contains 5 sequences with maximum_length=12 . Real lengths of each sequence are [2, 8, 5] . We can access x content via PaddedBatch properties x.payload and x.seq_lens . Real data have sequences are padded with zeros. We can imitate it with x.seq_len_mask . It return tensor with 1 if a position inside corresponded seq_len and 0 if position outside. Let's check out example >>> x.seq_len_mask Out: tensor([[1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0]]) There are 2, 8 and 5 valid tokens in lines. More way of seq_len_mask usage are in PaddedBatch docstring. We can recreate our x with modified content: x = PaddedBatch({k: v * x.seq_len_mask for k, v in x.payload.items()}, x.seq_lens) Now we can check x.payload and see features looks like real padded data: >>> x.payload['mcc_code'] Out: tensor([[8, 1, 0, 0, 0, 0, 0, 0], [5, 5, 9, 9, 4, 9, 3, 1], [4, 2, 2, 3, 3, 0, 0, 0]]) All invalid tokens are replaced with zeros. Generally, all layers respect PaddedBatch.seq_lens and no explicit zeroing of padded characters is required. ptls.trx_encoder.TrxEncoder Now we have an input data: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) And se can define a TrxEncoder model = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) We should provide feature description to TrxEncoder . Dictionary size and embedding size for categorical features. Scaler name for numerical features. identity means no rescaling. TrxEncoder concatenate all feature embeddings, sow output embedding size will be 6 + 2 + 1 . You may get output size from TrxEncoder with property: >>> model.output_size Out[]: 6 Let's transform our features to embeddings z = model(x) z is also PaddedBatch . z.seq_lens equals x.seq_lens . z.payload isn't dict, it's tensor of shape (B, T, H). In our example B, T = 3, 8 is input feature shape, H = 6 is output size of model. Now we can use other layers which consume transactional embeddings. Classes See docstrings for classes: - ptls.trx_encoder.PaddedBatch - ptls.trx_encoder.TrxEncoder","title":"trx_encoder"},{"location":"trx_encoder/#ptlstrx_encoder-usage","text":"","title":"ptls.trx_encoder usage"},{"location":"trx_encoder/#usage","text":"ptls.trx_encoder helps to make a representation for single transactions.","title":"Usage"},{"location":"trx_encoder/#ptlstrx_encoderpaddedbatch","text":"Input data is a raw feature formats. You can transform your transaction to correct format with ptls.data module. Some description about this process are here data_preparation.md Input data are covered in ptls.trx_encoder.PaddedBatch class. We can create PaddedBatch object manually for demo and test purposes. x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) Here x contains three features. Two are categorical and one is numerical: - mcc_code is categorical with dictionary_size=10 - currency is categorical with dictionary_size=4 - amount is numerical with mean=5 and std=4 x contains 5 sequences with maximum_length=12 . Real lengths of each sequence are [2, 8, 5] . We can access x content via PaddedBatch properties x.payload and x.seq_lens . Real data have sequences are padded with zeros. We can imitate it with x.seq_len_mask . It return tensor with 1 if a position inside corresponded seq_len and 0 if position outside. Let's check out example >>> x.seq_len_mask Out: tensor([[1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0]]) There are 2, 8 and 5 valid tokens in lines. More way of seq_len_mask usage are in PaddedBatch docstring. We can recreate our x with modified content: x = PaddedBatch({k: v * x.seq_len_mask for k, v in x.payload.items()}, x.seq_lens) Now we can check x.payload and see features looks like real padded data: >>> x.payload['mcc_code'] Out: tensor([[8, 1, 0, 0, 0, 0, 0, 0], [5, 5, 9, 9, 4, 9, 3, 1], [4, 2, 2, 3, 3, 0, 0, 0]]) All invalid tokens are replaced with zeros. Generally, all layers respect PaddedBatch.seq_lens and no explicit zeroing of padded characters is required.","title":"ptls.trx_encoder.PaddedBatch"},{"location":"trx_encoder/#ptlstrx_encodertrxencoder","text":"Now we have an input data: x = PaddedBatch( payload={ 'mcc_code': torch.randint(1, 10, (3, 8)), 'currency': torch.randint(1, 4, (3, 8)), 'amount': torch.randn(3, 8) * 4 + 5, }, length=torch.Tensor([2, 8, 5]).long() ) And se can define a TrxEncoder model = TrxEncoder( embeddings={ 'mcc_code': {'in': 10, 'out': 6}, 'currency': {'in': 4, 'out': 2}, }, numeric_values={'amount': 'identity'}, ) We should provide feature description to TrxEncoder . Dictionary size and embedding size for categorical features. Scaler name for numerical features. identity means no rescaling. TrxEncoder concatenate all feature embeddings, sow output embedding size will be 6 + 2 + 1 . You may get output size from TrxEncoder with property: >>> model.output_size Out[]: 6 Let's transform our features to embeddings z = model(x) z is also PaddedBatch . z.seq_lens equals x.seq_lens . z.payload isn't dict, it's tensor of shape (B, T, H). In our example B, T = 3, 8 is input feature shape, H = 6 is output size of model. Now we can use other layers which consume transactional embeddings.","title":"ptls.trx_encoder.TrxEncoder"},{"location":"trx_encoder/#classes","text":"See docstrings for classes: - ptls.trx_encoder.PaddedBatch - ptls.trx_encoder.TrxEncoder","title":"Classes"}]}