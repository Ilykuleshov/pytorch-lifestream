{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7431993",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a798aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../../data’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  239M  100  239M    0     0  52.9M      0  0:00:04  0:00:04 --:--:-- 52.9M\n",
      "Archive:  age-prediction-nti-sbebank-2019.zip\n",
      "  inflating: ../../data/test.csv     \n",
      "  inflating: ../../data/small_group_description.csv  \n",
      "  inflating: ../../data/train_target.csv  \n",
      "  inflating: ../../data/transactions_train.csv  \n",
      "  inflating: ../../data/transactions_test.csv  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('data/transactions_train.csv'):\n",
    "    ! mkdir -p data\n",
    "    ! curl -OL https://storage.yandexcloud.net/di-datasets/age-prediction-nti-sbebank-2019.zip\n",
    "    ! unzip -j -o age-prediction-nti-sbebank-2019.zip 'data/*.csv' -d data\n",
    "    ! mv age-prediction-nti-sbebank-2019.zip data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf847b53",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587df1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f989bc",
   "metadata": {},
   "source": [
    "## Pyspark Data Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1608b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data_path = 'data/'\n",
    "\n",
    "spark_conf = pyspark.SparkConf()\n",
    "spark_conf.setMaster(\"local[*]\").setAppName(\"PysparkDataPreprocessor\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.cores.max\", \"24\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27115f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c7afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    cols_event_time='trans_date',\n",
    "    time_transformation='float',\n",
    "    cols_category=[\"trans_date\", \"small_group\"],\n",
    "    cols_log_norm=[\"amount_rur\"],\n",
    "    cols_identity=[],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d43a15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_pysparkdf = preprocessor.fit_transform(source_data).persist()\n",
    "dataset_pysparkdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7582b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|client_id|          trans_date|         small_group|          amount_rur|          event_time|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      463|[726, 724, 725, 7...|[7, 1, 7, 3, 3, 2...|[0.32135927726085...|[1.0, 2.0, 5.0, 7...|\n",
      "|      471|[730, 726, 726, 7...|[68, 5, 4, 7, 1, ...|[0.26422890225828...|[0.0, 1.0, 1.0, 1...|\n",
      "|      496|[723, 723, 722, 7...|[3, 1, 1, 1, 1, 4...|[0.27119266729746...|[3.0, 3.0, 4.0, 5...|\n",
      "|      833|[726, 726, 726, 7...|[17, 15, 1, 49, 3...|[0.28922101008224...|[1.0, 1.0, 1.0, 2...|\n",
      "|     1238|[730, 723, 716, 7...|[3, 11, 3, 3, 1, ...|[0.13326234667635...|[0.0, 3.0, 9.0, 1...|\n",
      "|     1342|[730, 722, 702, 7...|[14, 3, 3, 21, 37...|[0.11547227625749...|[0.0, 4.0, 11.0, ...|\n",
      "|     1591|[724, 724, 723, 7...|[37, 20, 7, 6, 1,...|[0.35099852868535...|[2.0, 2.0, 3.0, 3...|\n",
      "|     1645|[724, 723, 723, 7...|[2, 16, 1, 1, 2, ...|[0.23653852058912...|[2.0, 3.0, 3.0, 4...|\n",
      "|     1829|[727, 716, 709, 7...|[3, 1, 3, 1, 1, 4...|[0.22955176609017...|[7.0, 9.0, 10.0, ...|\n",
      "|     1959|[723, 723, 722, 7...|[3, 1, 1, 1, 30, ...|[0.29358422166110...|[3.0, 3.0, 4.0, 5...|\n",
      "|     2122|[730, 722, 725, 7...|[38, 5, 14, 6, 3,...|[0.00341230998274...|[0.0, 4.0, 5.0, 5...|\n",
      "|     2142|[724, 724, 691, 6...|[7, 10, 1, 24, 11...|[0.29000320625500...|[2.0, 2.0, 13.0, ...|\n",
      "|     2659|[684, 688, 681, 6...|[3, 3, 36, 3, 1, ...|[0.25963118799171...|[27.0, 31.0, 39.0...|\n",
      "|     3175|[728, 728, 727, 7...|[4, 7, 1, 1, 2, 2...|[0.25272554086981...|[6.0, 6.0, 7.0, 1...|\n",
      "|     3749|[726, 722, 722, 7...|[3, 16, 35, 46, 1...|[0.23823234099543...|[1.0, 4.0, 4.0, 4...|\n",
      "|     3794|[723, 723, 722, 7...|[5, 10, 26, 26, 7...|[0.24852798380522...|[3.0, 3.0, 4.0, 7...|\n",
      "|     3997|[730, 726, 724, 7...|[3, 1, 29, 1, 1, ...|[0.35715492952800...|[0.0, 1.0, 2.0, 2...|\n",
      "|     4101|[723, 722, 725, 7...|[1, 1, 1, 13, 1, ...|[0.22094081440301...|[3.0, 4.0, 5.0, 6...|\n",
      "|     4818|[726, 724, 722, 7...|[1, 6, 6, 23, 6, ...|[0.26493123311313...|[1.0, 2.0, 4.0, 6...|\n",
      "|     4935|[723, 722, 722, 7...|[31, 31, 3, 10, 4...|[0.47558022334646...|[3.0, 4.0, 4.0, 4...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_pysparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d35e3205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('client_id', 'int'),\n",
       " ('trans_date', 'array<int>'),\n",
       " ('small_group', 'array<int>'),\n",
       " ('amount_rur', 'array<double>'),\n",
       " ('event_time', 'array<float>')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pysparkdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3510707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test dataset: 6077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset 21535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of valid dataset 2388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df = dataset_pysparkdf.sample(fraction=0.2)\n",
    "train_df = dataset_pysparkdf.subtract(test_df)\n",
    "\n",
    "valid_df = train_df.sample(fraction=0.1)\n",
    "train_df = train_df.subtract(valid_df)\n",
    "\n",
    "print('Size of test dataset:', test_df.count())\n",
    "print('Size of train dataset', train_df.count())\n",
    "print('Size of valid dataset', valid_df.count())\n",
    "\n",
    "test_df.write.parquet('test.parquet')\n",
    "train_df.write.parquet('train.parquet')\n",
    "valid_df.write.parquet('valid.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9080a6f",
   "metadata": {},
   "source": [
    "## Embedding training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56493c0b",
   "metadata": {},
   "source": [
    "Model training in our framework organised via pytorch-lightning (pl) framework.\n",
    "The key parts of neural networks training in pl are: \n",
    "\n",
    "    * model (pl.LightningModule)\n",
    "    * data_module (pl.LightningDataModule)\n",
    "    * pl.trainer (pl.trainer)\n",
    "    \n",
    "For futher details check https://www.pytorchlightning.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6ee58",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "988c508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.seq_encoder import SequenceEncoder\n",
    "from ptls.models import Head\n",
    "from ptls.lightning_modules.emb_module import EmbModule\n",
    "\n",
    "seq_encoder = SequenceEncoder(\n",
    "    category_features=preprocessor.get_category_sizes(),\n",
    "    numeric_features=[\"amount_rur\"],\n",
    "    trx_embedding_noize=0.003\n",
    ")\n",
    "\n",
    "head = Head(input_size=seq_encoder.embedding_size, use_norm_encoder=True)\n",
    "\n",
    "model = EmbModule(seq_encoder=seq_encoder, head=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87997ac0",
   "metadata": {},
   "source": [
    "### Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7d370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.data_module.emb_data_module import EmbeddingTrainDataModule\n",
    "\n",
    "dm_parquet = EmbeddingTrainDataModule(\n",
    "    pl_module=model,\n",
    "    parquet_train_path='train.parquet',\n",
    "    parquet_valid_path='valid.parquet',\n",
    "    min_seq_len=25,\n",
    "    seq_split_strategy='SampleSlices',\n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    "    split_count=5,\n",
    "    split_cnt_min=25,\n",
    "    split_cnt_max=200,\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    "    valid_num_workers=16,\n",
    "    valid_batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a09be",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fdbb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "# logging.getLogger(\"lightning\").addHandler(logging.NullHandler())\n",
    "# logging.getLogger(\"lightning\").propagate = False\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "#     progress_bar_refresh_rate=0,\n",
    "    max_epochs=150,\n",
    "    gpus=1 if torch.cuda.is_available() else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88078a3",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f40877df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(model, dm_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65b5e3",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25ab3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train = train_df.toPandas().to_dict('records')\n",
    "for d in train:\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, list):\n",
    "            d[k] = np.array(v)\n",
    "\n",
    "test = test_df.toPandas().to_dict('records')\n",
    "for d in test:\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, list):\n",
    "            d[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c32741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding inference\n",
    "\n",
    "from ptls.inference import get_embeddings\n",
    "\n",
    "train_embeds = get_embeddings(\n",
    "    data=train,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "test_embeds = get_embeddings(\n",
    "    data=test,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18245f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 514) (6000, 514)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_504</th>\n",
       "      <th>embed_505</th>\n",
       "      <th>embed_506</th>\n",
       "      <th>embed_507</th>\n",
       "      <th>embed_508</th>\n",
       "      <th>embed_509</th>\n",
       "      <th>embed_510</th>\n",
       "      <th>embed_511</th>\n",
       "      <th>client_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.111126</td>\n",
       "      <td>-0.275824</td>\n",
       "      <td>0.391841</td>\n",
       "      <td>-0.283697</td>\n",
       "      <td>-0.182938</td>\n",
       "      <td>0.108049</td>\n",
       "      <td>-0.181098</td>\n",
       "      <td>0.288231</td>\n",
       "      <td>0.022933</td>\n",
       "      <td>-0.302062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234748</td>\n",
       "      <td>-0.033312</td>\n",
       "      <td>-0.767539</td>\n",
       "      <td>-0.175905</td>\n",
       "      <td>0.021996</td>\n",
       "      <td>0.242000</td>\n",
       "      <td>0.145899</td>\n",
       "      <td>-0.308390</td>\n",
       "      <td>48529</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.175665</td>\n",
       "      <td>-0.219268</td>\n",
       "      <td>0.374399</td>\n",
       "      <td>-0.112002</td>\n",
       "      <td>-0.210926</td>\n",
       "      <td>0.093951</td>\n",
       "      <td>-0.269225</td>\n",
       "      <td>0.300919</td>\n",
       "      <td>-0.074439</td>\n",
       "      <td>-0.253708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203787</td>\n",
       "      <td>-0.072702</td>\n",
       "      <td>-0.813233</td>\n",
       "      <td>-0.135287</td>\n",
       "      <td>0.039629</td>\n",
       "      <td>0.253809</td>\n",
       "      <td>0.585902</td>\n",
       "      <td>-0.365863</td>\n",
       "      <td>48852</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0 -0.111126 -0.275824  0.391841 -0.283697 -0.182938  0.108049 -0.181098   \n",
       "1 -0.175665 -0.219268  0.374399 -0.112002 -0.210926  0.093951 -0.269225   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_504  embed_505  embed_506  \\\n",
       "0  0.288231  0.022933 -0.302062  ...   0.234748  -0.033312  -0.767539   \n",
       "1  0.300919 -0.074439 -0.253708  ...   0.203787  -0.072702  -0.813233   \n",
       "\n",
       "   embed_507  embed_508  embed_509  embed_510  embed_511  client_id  target  \n",
       "0  -0.175905   0.021996   0.242000   0.145899  -0.308390      48529       3  \n",
       "1  -0.135287   0.039629   0.253809   0.585902  -0.365863      48852       3  \n",
       "\n",
       "[2 rows x 514 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# join target and embeddings\n",
    "\n",
    "df_target = pd.read_csv(os.path.join(data_path, 'train_target.csv'))\n",
    "df_target = df_target.set_index('client_id')\n",
    "df_target.rename(columns={\"bins\": \"target\"}, inplace=True)\n",
    "\n",
    "train_df = pd.DataFrame(data=train_embeds, columns=[f'embed_{i}' for i in range(train_embeds.shape[1])])\n",
    "train_df['client_id'] = [x['client_id'] for x in train]\n",
    "train_df = train_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "test_df = pd.DataFrame(data=test_embeds, columns=[f'embed_{i}' for i in range(test_embeds.shape[1])])\n",
    "test_df['client_id'] = [x['client_id'] for x in test]\n",
    "test_df = test_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafa0c0",
   "metadata": {},
   "source": [
    "Obtained embeddings can be used as features for model training\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e3de46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6263"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embed_columns = [x for x in train_df.columns if x.startswith('embed')]\n",
    "x_train, y_train = train_df[embed_columns], train_df['target']\n",
    "x_test, y_test = test_df[embed_columns], test_df['target']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
