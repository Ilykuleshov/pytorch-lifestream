{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ddef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5251/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587df1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7431993",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a798aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../../data’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  239M  100  239M    0     0  43.6M      0  0:00:05  0:00:05 --:--:-- 50.2M\n",
      "Archive:  age-prediction-nti-sbebank-2019.zip\n",
      "  inflating: ../../data/test.csv     \n",
      "  inflating: ../../data/small_group_description.csv  \n",
      "  inflating: ../../data/train_target.csv  \n",
      "  inflating: ../../data/transactions_train.csv  \n",
      "  inflating: ../../data/transactions_test.csv  \n"
     ]
    }
   ],
   "source": [
    "! mkdir ../../data\n",
    "! curl -OL https://storage.yandexcloud.net/di-datasets/age-prediction-nti-sbebank-2019.zip\n",
    "! unzip -j -o age-prediction-nti-sbebank-2019.zip 'data/*.csv' -d ../../data\n",
    "! mv age-prediction-nti-sbebank-2019.zip ../../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f989bc",
   "metadata": {},
   "source": [
    "## Pyspark Data Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1608b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/mikheev/.local/share/virtualenvs/pytorch-lifestream-sv0mOraz/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/19 08:27:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/19 08:27:12 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.memoryOverhead', '4g'),\n",
       " ('spark.app.id', 'local-1650356833093'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/mnt/mikheev/pytorch-lifestream/demo/spark-warehouse'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.driver.port', '41019'),\n",
       " ('spark.executor.memory', '16g'),\n",
       " ('spark.app.startTime', '1650356832091'),\n",
       " ('spark.local.dir', '../../spark_cache'),\n",
       " ('spark.driver.host', 'tdl3.internal.cloudapp.net'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PysparkDataPreprocessor'),\n",
       " ('spark.cores.max', '24'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g'),\n",
       " ('spark.executor.memoryOverhead', '4g')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "\n",
    "data_path = '../../data/'\n",
    "\n",
    "spark_conf = pyspark.SparkConf()\n",
    "spark_conf.setMaster(\"local[*]\").setAppName(\"PysparkDataPreprocessor\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.cores.max\", \"24\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27115f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+----------+\n",
      "|client_id|trans_date|small_group|amount_rur|\n",
      "+---------+----------+-----------+----------+\n",
      "|    33172|         6|          4|    71.463|\n",
      "|    33172|         6|         35|    45.017|\n",
      "+---------+----------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'transactions_train.csv'))\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c7afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    cols_event_time='trans_date',\n",
    "    time_transformation='float',\n",
    "    cols_category=[\"trans_date\", \"small_group\"],\n",
    "    cols_log_norm=[\"amount_rur\"],\n",
    "    cols_identity=[],\n",
    "    print_dataset_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43a15ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/19 08:27:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/04/19 08:27:50 WARN CacheManager: Asked to cache already cached data.        \n",
      "[Stage 19:====================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 475 ms, sys: 226 ms, total: 701 ms\n",
      "Wall time: 29.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_pysparkdf = preprocessor.fit_transform(source_data).persist()\n",
    "dataset_pysparkdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7582b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|client_id|          trans_date|         small_group|          amount_rur|          event_time|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      463|[726, 724, 725, 7...|[7, 1, 7, 3, 3, 2...|[0.32135927726085...|[1.0, 2.0, 5.0, 7...|\n",
      "|      471|[730, 726, 726, 7...|[68, 5, 4, 7, 1, ...|[0.26422890225828...|[0.0, 1.0, 1.0, 1...|\n",
      "|      496|[723, 723, 722, 7...|[3, 1, 1, 1, 1, 4...|[0.27119266729746...|[3.0, 3.0, 4.0, 5...|\n",
      "|      833|[726, 726, 726, 7...|[17, 15, 1, 49, 3...|[0.28922101008224...|[1.0, 1.0, 1.0, 2...|\n",
      "|     1238|[730, 723, 716, 7...|[3, 11, 3, 3, 1, ...|[0.13326234667635...|[0.0, 3.0, 9.0, 1...|\n",
      "|     1342|[730, 722, 702, 7...|[14, 3, 3, 21, 37...|[0.11547227625749...|[0.0, 4.0, 11.0, ...|\n",
      "|     1591|[724, 724, 723, 7...|[37, 20, 7, 6, 1,...|[0.35099852868535...|[2.0, 2.0, 3.0, 3...|\n",
      "|     1645|[724, 723, 723, 7...|[2, 16, 1, 1, 2, ...|[0.23653852058912...|[2.0, 3.0, 3.0, 4...|\n",
      "|     1829|[727, 716, 709, 7...|[3, 1, 3, 1, 1, 4...|[0.22955176609017...|[7.0, 9.0, 10.0, ...|\n",
      "|     1959|[723, 723, 722, 7...|[3, 1, 1, 1, 30, ...|[0.29358422166110...|[3.0, 3.0, 4.0, 5...|\n",
      "|     2122|[730, 722, 725, 7...|[38, 5, 14, 6, 3,...|[0.00341230998274...|[0.0, 4.0, 5.0, 5...|\n",
      "|     2142|[724, 724, 691, 6...|[7, 10, 1, 24, 11...|[0.29000320625500...|[2.0, 2.0, 13.0, ...|\n",
      "|     2659|[684, 688, 681, 6...|[3, 3, 36, 3, 1, ...|[0.25963118799171...|[27.0, 31.0, 39.0...|\n",
      "|     3175|[728, 728, 727, 7...|[4, 7, 1, 1, 2, 2...|[0.25272554086981...|[6.0, 6.0, 7.0, 1...|\n",
      "|     3749|[726, 722, 722, 7...|[3, 16, 35, 46, 1...|[0.23823234099543...|[1.0, 4.0, 4.0, 4...|\n",
      "|     3794|[723, 723, 722, 7...|[5, 10, 26, 26, 7...|[0.24852798380522...|[3.0, 3.0, 4.0, 7...|\n",
      "|     3997|[730, 726, 724, 7...|[3, 1, 29, 1, 1, ...|[0.35715492952800...|[0.0, 1.0, 2.0, 2...|\n",
      "|     4101|[723, 722, 725, 7...|[1, 1, 1, 13, 1, ...|[0.22094081440301...|[3.0, 4.0, 5.0, 6...|\n",
      "|     4818|[726, 724, 722, 7...|[1, 6, 6, 23, 6, ...|[0.26493123311313...|[1.0, 2.0, 4.0, 6...|\n",
      "|     4935|[723, 722, 722, 7...|[31, 31, 3, 10, 4...|[0.47558022334646...|[3.0, 4.0, 4.0, 4...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_pysparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d35e3205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('client_id', 'int'),\n",
       " ('trans_date', 'array<int>'),\n",
       " ('small_group', 'array<int>'),\n",
       " ('amount_rur', 'array<double>'),\n",
       " ('event_time', 'array<float>')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pysparkdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3510707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of whole dataset: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test dataset: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset 21600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of valid dataset 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "N = dataset_pysparkdf.count()\n",
    "print('Size of whole dataset:', N)\n",
    "\n",
    "test_df = dataset_pysparkdf.limit(int(0.2 * N))\n",
    "train_df = dataset_pysparkdf.subtract(test_df)\n",
    "\n",
    "N_train = train_df.count()\n",
    "valid_df = train_df.limit(int(0.1 * N_train))\n",
    "train_df = train_df.subtract(valid_df)\n",
    "\n",
    "print('Size of test dataset:', test_df.count())\n",
    "print('Size of train dataset', train_df.count())\n",
    "print('Size of valid dataset', valid_df.count())\n",
    "\n",
    "test_df.write.parquet('test.parquet')\n",
    "train_df.write.parquet('train.parquet')\n",
    "valid_df.write.parquet('valid.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9080a6f",
   "metadata": {},
   "source": [
    "## Embedding training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56493c0b",
   "metadata": {},
   "source": [
    "Model training in our framework organised via pytorch-lightning (pl) framework.\n",
    "The key parts of neural networks training in pl are: \n",
    "\n",
    "    * model (pl.LightningModule)\n",
    "    * data_module (pl.LightningDataModule)\n",
    "    * pl.trainer (pl.trainer)\n",
    "    \n",
    "For futher details check https://www.pytorchlightning.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6ee58",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988c508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.seq_encoder import SequenceEncoder\n",
    "from dltranz.models import Head\n",
    "from dltranz.lightning_modules.emb_module import EmbModule\n",
    "\n",
    "seq_encoder = SequenceEncoder(\n",
    "    category_features=preprocessor.get_category_sizes(),\n",
    "    numeric_features=[\"amount_rur\"],\n",
    "    trx_embedding_noize=0.003\n",
    ")\n",
    "\n",
    "head = Head(input_size=seq_encoder.embedding_size, use_norm_encoder=True)\n",
    "\n",
    "model = EmbModule(seq_encoder=seq_encoder, head=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87997ac0",
   "metadata": {},
   "source": [
    "### Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7d370b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltranz.data_load.data_module.emb_data_module import EmbeddingTrainDataModule\n",
    "\n",
    "dm_parquet = EmbeddingTrainDataModule(\n",
    "    pl_module=model,\n",
    "    parquet_train_path='train.parquet',\n",
    "    parquet_valid_path='valid.parquet',\n",
    "    min_seq_len=25,\n",
    "    seq_split_strategy='SampleSlices',\n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    "    split_count=5,\n",
    "    split_cnt_min=25,\n",
    "    split_cnt_max=200,\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    "    valid_num_workers=16,\n",
    "    valid_batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a09be",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fdbb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "# logging.getLogger(\"lightning\").addHandler(logging.NullHandler())\n",
    "# logging.getLogger(\"lightning\").propagate = False\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "#     progress_bar_refresh_rate=0,\n",
    "    max_epochs=150,\n",
    "    gpus=1 if torch.cuda.is_available() else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88078a3",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40877df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(model, dm_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65b5e3",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25ab3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train = train_df.toPandas().to_dict('records')\n",
    "for d in train:\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, list):\n",
    "            d[k] = np.array(v)\n",
    "\n",
    "test = test_df.toPandas().to_dict('records')\n",
    "for d in test:\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, list):\n",
    "            d[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c32741d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21600, 512), (6000, 512))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding inference\n",
    "\n",
    "from dltranz.inference import get_embeddings\n",
    "\n",
    "train_embeds = get_embeddings(\n",
    "    data=train,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "test_embeds = get_embeddings(\n",
    "    data=test,\n",
    "    model=model, \n",
    "    category_names = model.seq_encoder.category_names,\n",
    "    category_max_size = model.seq_encoder.category_max_size,\n",
    ")\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18245f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 514) (6000, 514)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_504</th>\n",
       "      <th>embed_505</th>\n",
       "      <th>embed_506</th>\n",
       "      <th>embed_507</th>\n",
       "      <th>embed_508</th>\n",
       "      <th>embed_509</th>\n",
       "      <th>embed_510</th>\n",
       "      <th>embed_511</th>\n",
       "      <th>client_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800337</td>\n",
       "      <td>0.058391</td>\n",
       "      <td>0.025423</td>\n",
       "      <td>-0.186267</td>\n",
       "      <td>0.035758</td>\n",
       "      <td>-0.237643</td>\n",
       "      <td>0.052041</td>\n",
       "      <td>-0.041745</td>\n",
       "      <td>0.517471</td>\n",
       "      <td>-0.240486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278272</td>\n",
       "      <td>-0.920710</td>\n",
       "      <td>-0.947493</td>\n",
       "      <td>-0.137431</td>\n",
       "      <td>-0.052345</td>\n",
       "      <td>0.014428</td>\n",
       "      <td>-0.975207</td>\n",
       "      <td>-0.129017</td>\n",
       "      <td>48529</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.977548</td>\n",
       "      <td>-0.066553</td>\n",
       "      <td>0.030726</td>\n",
       "      <td>-0.156769</td>\n",
       "      <td>-0.252864</td>\n",
       "      <td>-0.275211</td>\n",
       "      <td>0.059669</td>\n",
       "      <td>-0.005294</td>\n",
       "      <td>0.546271</td>\n",
       "      <td>0.749741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238649</td>\n",
       "      <td>-0.956998</td>\n",
       "      <td>-0.916645</td>\n",
       "      <td>-0.119243</td>\n",
       "      <td>-0.045673</td>\n",
       "      <td>0.026230</td>\n",
       "      <td>-0.968116</td>\n",
       "      <td>0.161983</td>\n",
       "      <td>48852</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0  0.800337  0.058391  0.025423 -0.186267  0.035758 -0.237643  0.052041   \n",
       "1  0.977548 -0.066553  0.030726 -0.156769 -0.252864 -0.275211  0.059669   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_504  embed_505  embed_506  \\\n",
       "0 -0.041745  0.517471 -0.240486  ...   0.278272  -0.920710  -0.947493   \n",
       "1 -0.005294  0.546271  0.749741  ...   0.238649  -0.956998  -0.916645   \n",
       "\n",
       "   embed_507  embed_508  embed_509  embed_510  embed_511  client_id  target  \n",
       "0  -0.137431  -0.052345   0.014428  -0.975207  -0.129017      48529       3  \n",
       "1  -0.119243  -0.045673   0.026230  -0.968116   0.161983      48852       3  \n",
       "\n",
       "[2 rows x 514 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join target and embeddings\n",
    "\n",
    "df_target = pd.read_csv(os.path.join(data_path, 'train_target.csv'))\n",
    "df_target = df_target.set_index('client_id')\n",
    "df_target.rename(columns={\"bins\": \"target\"}, inplace=True)\n",
    "\n",
    "train_df = pd.DataFrame(data=train_embeds, columns=[f'embed_{i}' for i in range(train_embeds.shape[1])])\n",
    "train_df['client_id'] = [x['client_id'] for x in train]\n",
    "train_df = train_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "test_df = pd.DataFrame(data=test_embeds, columns=[f'embed_{i}' for i in range(test_embeds.shape[1])])\n",
    "test_df['client_id'] = [x['client_id'] for x in test]\n",
    "test_df = test_df.merge(df_target, how='left', on='client_id')\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafa0c0",
   "metadata": {},
   "source": [
    "Obtained embeddings can be used as features for model training\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37e3de46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6221666666666666"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embed_columns = [x for x in train_df.columns if x.startswith('embed')]\n",
    "x_train, y_train = train_df[embed_columns], train_df['target']\n",
    "x_test, y_test = test_df[embed_columns], test_df['target']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5b0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
