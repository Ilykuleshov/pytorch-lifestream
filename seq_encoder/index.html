<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://example.com/seq_encoder/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>seq_encoder - PyTorch-LifeStream</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "seq_encoder";
        var mkdocs_page_input_path = "seq_encoder.md";
        var mkdocs_page_url = "/seq_encoder/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> PyTorch-LifeStream
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guide</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../data_preparation/">Data Preparation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../embeddings_with_other_losses/">Losses</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">seq_encoder</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#usage">Usage</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#embeddings-as-input">Embeddings as input</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#features-as-input">Features as input</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#classes">Classes</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../trx_encoder/">trx_encoder</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">PyTorch-LifeStream</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>User Guide &raquo;</li><li>seq_encoder</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="ptlsseq_encoder-usage"><code>ptls.seq_encoder</code> usage</h1>
<h2 id="usage">Usage</h2>
<p><code>trx_encoder</code> works with individual transaction.
<code>seq_encoder</code> takes into account sequential structure and the links between transactions.</p>
<p>There are 2 types of seq encoders:
- required embeddings as input
- requires raw features as input</p>
<h3 id="embeddings-as-input">Embeddings as input</h3>
<p>We implement ptls-api for <code>torch</code> sequential layers:
- <code>ptls.seq_encoder.RnnEncoder</code> for <code>torch.nn.GRU</code>
- <code>ptls.seq_encoder.TransformerEncoder</code> for <code>torch.nn.TransformerEncoder</code></p>
<p>They expect vectorized input, which can be obtained with <code>TrxEncoder</code>.</p>
<p>Output format controlled by <code>is_reduce_sequence</code> property. <code>True</code> means that sequence will be reduced 
to one single vector.  It's last hidden state for RNN and CLS token output for transformer.
<code>False</code> means than all hidden vectors for all transactions will be returned.  Set this property based on your needs.
It's possible to set it during encoder initialisation. It's possible to change it in runtime.</p>
<p>Simple Example:</p>
<pre><code class="language-python">x = PaddedBatch(torch.randn(10, 80, 4), torch.randint(40, 80, (10,)))
seq_encoder = RnnEncoder(input_size=4, hidden_size=16)
y = seq_encoder(x)
assert y.payload.size() == (10, 80, 16)
</code></pre>
<p>More complicated example:</p>
<pre><code class="language-python">x = PaddedBatch(
    payload={
        'mcc_code': torch.randint(1, 10, (3, 8)),
        'currency': torch.randint(1, 4, (3, 8)),
        'amount': torch.randn(3, 8) * 4 + 5,
    },
    length=torch.Tensor([2, 8, 5]).long()
)

trx_encoder = TrxEncoder(
    embeddings={
        'mcc_code': {'in': 10, 'out': 6},
        'currency': {'in': 4, 'out': 2},
    },
    numeric_values={'amount': 'identity'},

)
seq_encoder = RnnEncoder(input_size=trx_encoder.output_size, hidden_size=16)

z = trx_encoder(x)
y = seq_encoder(z)  # embeddings wor each transaction
seq_encoder.is_reduce_sequence = True
h = seq_encoder(z)  # embeddings for sequences, aggregate all transactions in one embedding

assert y.payload.size() == (3, 8, 16)
assert h.size() == (3, 16)
</code></pre>
<p>Usually <code>seq_encoder</code> used with preliminary <code>trx_encoder</code>. It's possible to pack them to <code>torch.nn.Sequential</code>.</p>
<p>It's possible to add more layers between <code>trx_encoder</code> and <code>seq_encoder</code> (linear, normalisation, convolutions, ...). 
They should work with PaddedBatch. Examples will be presented later. Such layers also works after <code>seq_encoder</code>
with <code>is_reduce_sequence=False</code>.</p>
<h3 id="features-as-input">Features as input</h3>
<p>As you can see <code>TrxEncoder</code> works with raw features and compatible with embedding seq encoder.
We make a composition layers, which contains <code>TrxEncoder</code> and one <code>SeqEncoder</code> implementation.
There are:
- <code>ptls.seq_encoder.RnnSeqEncoder</code> with <code>RnnEncoder</code>
- <code>ptls.seq_encoder.TransformerSeqEncoder</code> with <code>TransformerEncoder</code></p>
<p>They work as simple <code>Sequential(trx_encoder, seq_encoder)</code> and support <code>is_reduce_sequence</code> property.
The main advantage that you can simply create such encoder from config file using <code>hydra instantiate</code> tools.
You can avoid of explicit set of <code>seq_encoder.input_size</code>, they will be taken from <code>trx_encoder</code>.  Let's compare.</p>
<p>Sequential-style:</p>
<pre><code class="language-python">config = &quot;&quot;&quot;
    model:
        _target_: torch.nn.Sequential
        _args_:
        - 
            _target_: ptls.trx_encoder.TrxEncoder
            embeddings:
                mcc_code:
                    in: 10
                    out: 6
                currency:
                    in: 4
                    out: 2
            numeric_values:
                amount: identity
        -
            _target_: ptls.seq_encoder.RnnEncoder
            input_size: 9  # depends on TrxEncoder output
            hidden_size: 24
&quot;&quot;&quot;
model = hydra.utils.instantiate(OmegaConf.create(config))['model']
</code></pre>
<p>SeqEncoder-style:</p>
<pre><code class="language-python">config = &quot;&quot;&quot;
    model:
        _target_: ptls.seq_encoder.RnnSeqEncoder
        trx_encoder:
            _target_: ptls.trx_encoder.TrxEncoder
            embeddings:
                mcc_code:
                    in: 10
                    out: 6
                currency:
                    in: 4
                    out: 2
            numeric_values:
                amount: identity
        hidden_size: 24
&quot;&quot;&quot;
model = hydra.utils.instantiate(OmegaConf.create(config))['model']
</code></pre>
<p>The second config are simpler. Both of configs make an identical model. You can check:</p>
<pre><code class="language-python">x = PaddedBatch(
    payload={
        'mcc_code': torch.randint(1, 10, (3, 8)),
        'currency': torch.randint(1, 4, (3, 8)),
        'amount': torch.randn(3, 8) * 4 + 5,
    },
    length=torch.Tensor([2, 8, 5]).long()
)

y = model(x)
</code></pre>
<p>Also we have <code>ptls.seq_encoder.AggFeatureSeqEncoder</code>.
It looks like seq_encoder. It take raw features at input and provide reduced representation at output.
This encoder creates features, which are good for boosting model. This is a strong baseline for many tasks.
<code>AggFeatureSeqEncoder</code> eat the same input as other seq_encoders, and it can easily be replaced
by rnn of transformer seq encoder.  It use gpu and works fast. It haven't parameters for learn.</p>
<p>Possible pipeline:</p>
<pre><code class="language-python">seq_encoder = AggFeatureSeqEncoder(...)
agg_embeddings = trainer.predict(seq_encoder, dataloader)
catboost_model.fit(agg_embeddings, target)
</code></pre>
<p>We plain to split <code>AggFeatureSeqEncoder</code> into components which will be compatible with other ptls-layers.
It will be possible to choose flexible between <code>TrxEncoder</code> with <code>AggSeqEncoder</code> and <code>OheEncoder</code> with <code>RnnEncoder</code>.</p>
<h2 id="classes">Classes</h2>
<p>See docstrings for classes.</p>
<p>Take trx embedding as input:
- <code>ptls.seq_encoder.RnnEncoder</code>
- <code>ptls.seq_encoder.TransformerEncoder</code></p>
<p>Take raw features as input:
- <code>ptls.seq_encoder.RnnSeqEncoder</code>
- <code>ptls.seq_encoder.TransformerSeqEncoder</code>
- <code>ptls.seq_encoder.AggFeatureSeqEncoder</code></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../embeddings_with_other_losses/" class="btn btn-neutral float-left" title="Losses"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../trx_encoder/" class="btn btn-neutral float-right" title="trx_encoder">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../embeddings_with_other_losses/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../trx_encoder/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
